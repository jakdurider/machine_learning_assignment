{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdx44bJMvN8D"
   },
   "source": [
    "# M2608.001300 Machine Learning<br> Assignment #3 Training Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: $: not found\n"
     ]
    }
   ],
   "source": [
    "!$ ./CollectSubmission.sh 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bnidpii8vN8J"
   },
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TG1TYobCvN8M"
   },
   "source": [
    "Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **TensorFlow**. <br>\n",
    "In this notebook, you will learn how to train convolutional neural networks (CNNs) for classifying images in the CIFAR-10 dataset. <br>\n",
    "There are **2 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "1. [Training a CNN model with Inception modules](#1)\n",
    "2. [Design a better model on CIFAR-10](#2)\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all three parts**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a compressed file called *team_#.tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage example: ./*CollectSubmission.sh* &nbsp; team_77)\n",
    "\n",
    "### Some helpful tutorials and references for assignment #3:\n",
    "- [1] TensorFlow official tutorials. [[link]](https://www.tensorflow.org/get_started/get_started)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Szegedy et al., \"Going deeper with convolutions\", CVPR 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sk5LwjHEvN8Q"
   },
   "source": [
    "## Load datasets\n",
    "The CIFAR-10 dataset will be downloaded automatically if it is not located in the *Utils* directory. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztZI0QM4vN8T"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10, plot_images\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "    \n",
    "conf = tf.ConfigProto()\n",
    "conf.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14277,
     "status": "ok",
     "timestamp": 1558516324361,
     "user": {
      "displayName": "이철민",
      "photoUrl": "",
      "userId": "06884576174768988809"
     },
     "user_tz": -540
    },
    "id": "_B5DS5UJvN8g",
    "outputId": "0a234849-6515-45dc-966b-0453114ae8c9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has already been downloaded and unpacked.\n",
      "Train data shape (40000, 32, 32, 3)\n",
      "Train labels shape (40000,)\n",
      "Validation data shape (10000, 32, 32, 3)\n",
      "Validataion labels shape (10000,)\n",
      "Test data shape (10000, 32, 32, 3)\n",
      "Test labels shape (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test, Class_names = load_CIFAR10()\n",
    "print('Train data shape ' + str(X_train.shape))\n",
    "print('Train labels shape ' + str(Y_train.shape))\n",
    "print('Validation data shape ' + str(X_val.shape))\n",
    "print('Validataion labels shape ' + str(Y_val.shape))\n",
    "print('Test data shape ' + str(X_test.shape))\n",
    "print('Test labels shape ' + str(Y_test.shape))\n",
    "\n",
    "\n",
    "plot_images(X_train, Y_train, Class_names, Each_Category=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1B4z1ai5vN82"
   },
   "source": [
    "## <a name=\"1\"></a>1. Training a CNN model with Inception module\n",
    "\n",
    "In this section, you will implement a little more complex CNN model with an `Inception module` [6]. <br>\n",
    "\n",
    "![Inception_module](utils/InceptionModule_GoogLeNet.png)\n",
    "\n",
    "Using the code provided as guidance, <br>\n",
    "**(1)** Define an `inception module`, which is the building block of **Inception model (a.k.a GoogLeNet)**, the winner of ILSVRC14. <br>\n",
    "**(2)** Define, train, and evaluate a CNN model with the following architecture and training setup:\n",
    "\n",
    "#### CNN architecture:\n",
    "* CNN model consists with stem layer, inception module, and fully connected layer\n",
    "* Stem layer with\n",
    "    * conv-pooling-conv-pooling\n",
    "    * 8 filters for the 9x9 convolutions\n",
    "    * 3x3 max pooling\n",
    "* `Inception module` with \n",
    "    * 8 filters for the main convolutions (blue blocks in the Figure(a))\n",
    "    * 3 filters for the dimensionality reduction convolutions (yellow blocks in the Figure(a))\n",
    "    *  ReLU activation\n",
    "* Fully connected layer with 10 output units and linear activation\n",
    "* Choose the proper padding option on your own.\n",
    "\n",
    "#### Training setup:\n",
    "* Loss function: Sotfmax cross entropy\n",
    "* Optimizer: Gradient descent with 0.01 learning rate\n",
    "* Batch size: 500\n",
    "* Training epoch: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7efMg8jvN85"
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9M1TsxyuvN9H"
   },
   "outputs": [],
   "source": [
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oTdQoT6vN9R"
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   use_pooling=True):  # Use 3x3 max-pooling.\n",
    "    \n",
    "    \n",
    "    \n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "#     shape = [filter_size, filter_size, 1, num_filters]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    \n",
    "    weights = new_weights(shape=shape)\n",
    "#     weights_list=[]\n",
    "#     for i in range(num_input_channels):\n",
    "#         weights_list=weights_list+new_weights(shape=shape)\n",
    "    \n",
    "#     weights=tf.concat(3,weights_list)\n",
    "        \n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "    \n",
    "\n",
    "    if use_pooling:\n",
    "        \n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 3, 3, 1],\n",
    "                               strides=[1, 3, 3, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oejKcYVvN9Y"
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbahTOvmvN9h"
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GOj5i1i-vN9q"
   },
   "outputs": [],
   "source": [
    "def Inception_module(Input, C1, C3_R, C3, C5_R, C5, P3_R):\n",
    "    '''\n",
    "    C1, C3, C5: number of filters for the main convolutions\n",
    "    C3_R, C5_R, P3_R: number of filters for the dimensionality reduction convolutions\n",
    "    '''\n",
    "    '''\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    '''\n",
    "    layer_C1 , weights_C1=new_conv_layer(input=Input,\n",
    "                           num_input_channels=int(Input.shape[3]),\n",
    "                           filter_size=1,\n",
    "                           num_filters=C1,\n",
    "                           use_pooling=False)\n",
    "    \n",
    "    layer_C3_R, weights_C3_R=new_conv_layer(input=Input,\n",
    "                                           num_input_channels=int(Input.shape[3]),\n",
    "                                           filter_size=1,\n",
    "                                           num_filters=C3_R,\n",
    "                                           use_pooling=False)\n",
    "    \n",
    "    layer_C3, weights_C3=new_conv_layer(input=layer_C3_R,\n",
    "                                       num_input_channels=C3_R,\n",
    "                                       filter_size=3,\n",
    "                                       num_filters=C3,\n",
    "                                       use_pooling=False)\n",
    "    \n",
    "    layer_C5_R, weights_C5_R=new_conv_layer(input=Input,\n",
    "                                           num_input_channels=int(Input.shape[3]),\n",
    "                                           filter_size=1,\n",
    "                                           num_filters=C5_R,\n",
    "                                           use_pooling=False)\n",
    "    \n",
    "    layer_C5, weights_C5=new_conv_layer(input=layer_C5_R,\n",
    "                                       num_input_channels=C5_R,\n",
    "                                       filter_size=5,\n",
    "                                       num_filters=C5,\n",
    "                                       use_pooling=False)\n",
    "    \n",
    "    layer_P3=tf.nn.max_pool(value=Input,\n",
    "                           ksize=[1,3,3,1],\n",
    "                           strides=[1,1,1,1],\n",
    "                           padding='SAME')\n",
    "    \n",
    "    layer_P3_R , weights_P3_R =new_conv_layer(input=layer_P3,\n",
    "                                             num_input_channels=int(layer_P3.shape[3]),\n",
    "                                             filter_size=1,\n",
    "                                             num_filters=P3_R,\n",
    "                                             use_pooling=False)\n",
    "    \n",
    "    Inception=tf.concat([layer_C1,layer_C3,layer_C5,layer_P3_R],axis=3)\n",
    "    \n",
    "    \n",
    "#     weights = [weights_C1, weights_C3_R, weights_C3, weights_C5_R, weights_C5, weights_P3_R]\n",
    "    \n",
    "    '''\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    '''\n",
    "    return Inception\n",
    "\n",
    "class inception_model(object):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "       \n",
    "        \n",
    "        self.Input_image=tf.placeholder(tf.float32,shape=[None,32,32,3],name='Input_image')\n",
    "        \n",
    "        self.stem_layer1 , self.stem_weights1=new_conv_layer(input=self.Input_image,\n",
    "                           num_input_channels=int(self.Input_image.shape[3]),\n",
    "                           filter_size=9,\n",
    "                           num_filters=8,\n",
    "                           use_pooling=True)\n",
    "        \n",
    "        self.stem_layer2 , self.stem_weights2=new_conv_layer(input=self.stem_layer1,\n",
    "                           num_input_channels=int(self.stem_layer1.shape[3]),\n",
    "                           filter_size=9,\n",
    "                           num_filters=8,\n",
    "                           use_pooling=True)\n",
    "        \n",
    "        self.inception = Inception_module(self.stem_layer2,8,3,8,3,8,3)\n",
    "        \n",
    "        self.layer_flat, self.num_features= flatten_layer(self.inception)\n",
    "        \n",
    "        self.layer_fc=new_fc_layer(input=self.layer_flat,\n",
    "                             num_inputs=self.num_features,\n",
    "                             num_outputs=10,\n",
    "                             use_relu=True)\n",
    "        \n",
    "        self.y_pred=tf.nn.softmax(self.layer_fc)\n",
    "        \n",
    "        self.y_pred_cls=tf.argmax(self.y_pred,axis=1)\n",
    "        \n",
    "        \n",
    "        self.y_true_cls=tf.placeholder(tf.int64,shape=[None,],name='y_true_cls')\n",
    "        \n",
    "        self.y_true=tf.one_hot(indices=self.y_true_cls,depth=10)\n",
    "        \n",
    "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.layer_fc,\n",
    "                                                        labels=self.y_true)\n",
    "        \n",
    "        self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(self.cost)\n",
    "        \n",
    "        self.correct_prediction = tf.equal(self.y_pred_cls, self.y_true_cls)\n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Inception_module2(Input, C1, C3_R, C3, C5_R, C5, P3_R):\n",
    "    '''\n",
    "    C1, C3, C5: number of filters for the main convolutions\n",
    "    C3_R, C5_R, P3_R: number of filters for the dimensionality reduction convolutions\n",
    "    '''\n",
    "    '''\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    '''\n",
    "    layer_C1 , weights_C1=new_conv_layer(input=Input,\n",
    "                           num_input_channels=int(Input.shape[3]),\n",
    "                           filter_size=1,\n",
    "                           num_filters=C1,\n",
    "                           use_pooling=False)\n",
    "    \n",
    "    layer_C3_R, weights_C3_R=new_conv_layer(input=Input,\n",
    "                                           num_input_channels=int(Input.shape[3]),\n",
    "                                           filter_size=1,\n",
    "                                           num_filters=C3_R,\n",
    "                                           use_pooling=False)\n",
    "    \n",
    "    layer_C3, weights_C3=new_conv_layer(input=layer_C3_R,\n",
    "                                       num_input_channels=C3_R,\n",
    "                                       filter_size=3,\n",
    "                                       num_filters=C3,\n",
    "                                       use_pooling=False)\n",
    "    \n",
    "    layer_C5_R, weights_C5_R=new_conv_layer(input=Input,\n",
    "                                           num_input_channels=int(Input.shape[3]),\n",
    "                                           filter_size=1,\n",
    "                                           num_filters=C5_R,\n",
    "                                           use_pooling=False)\n",
    "    \n",
    "    layer_C5, weights_C5=new_conv_layer(input=layer_C5_R,\n",
    "                                       num_input_channels=C5_R,\n",
    "                                       filter_size=3,\n",
    "                                       num_filters=C5,\n",
    "                                       use_pooling=False)\n",
    "    \n",
    "    layer_P3=tf.nn.max_pool(value=Input,\n",
    "                           ksize=[1,3,3,1],\n",
    "                           strides=[1,1,1,1],\n",
    "                           padding='SAME')\n",
    "    \n",
    "    layer_P3_R , weights_P3_R =new_conv_layer(input=layer_P3,\n",
    "                                             num_input_channels=int(layer_P3.shape[3]),\n",
    "                                             filter_size=1,\n",
    "                                             num_filters=P3_R,\n",
    "                                             use_pooling=False)\n",
    "    \n",
    "    Inception=tf.concat([layer_C1,layer_C3,layer_C5,layer_P3_R],axis=3)\n",
    "    \n",
    "    \n",
    "    weights = [weights_C1, weights_C3_R, weights_C3, weights_C5_R, weights_C5, weights_P3_R]\n",
    "    \n",
    "    '''\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    '''\n",
    "    return (Inception, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6qWno-RvN9z"
   },
   "outputs": [],
   "source": [
    "# Create a function to train and evaluata a model\n",
    "# You can reuse this function throughout the assignment\n",
    "def run_model(session, model, X, Y, epochs=1, batch_size=500, is_training=False):\n",
    "    # For training the model\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if is_training:\n",
    "        for i in range(epochs):\n",
    "\n",
    "            idx=np.random.randint(X.shape[0],size=batch_size)\n",
    "        \n",
    "            \n",
    "            X_train_batch=X[idx,:]\n",
    "            \n",
    "            Y_train_batch=Y[idx]\n",
    "            \n",
    "            feed_dict_train = {model.Input_image: X_train_batch,\n",
    "                               \n",
    "                               model.y_true_cls: Y_train_batch}\n",
    "\n",
    "            session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    " # Ending time.\n",
    "        end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "        time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "        print(\"#Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "        print(\"#Training done!\")\n",
    "\n",
    "    # Evaluate loss and accuracy of the model\n",
    "    else:\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "#         loss=model.cross_entropy\n",
    "        \n",
    "#         accuracy=model.correct_prediction\n",
    "        \n",
    "       \n",
    "\n",
    "        feed_dict = {model.Input_image: X,\n",
    "                       model.y_true_cls: Y}\n",
    "        \n",
    "        accuracy = session.run(model.accuracy, feed_dict = feed_dict)\n",
    "         \n",
    "        loss = session.run(model.cost, feed_dict = feed_dict)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         accuracy=int(accuracy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "        return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1935
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45193,
     "status": "error",
     "timestamp": 1558516355296,
     "user": {
      "displayName": "이철민",
      "photoUrl": "",
      "userId": "06884576174768988809"
     },
     "user_tz": -540
    },
    "id": "dx7VcDPvvN99",
    "outputId": "10cd0c02-238e-4c5c-c95e-a7cfbff923c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Time usage: 0:00:00\n",
      "#Training done!\n",
      "(Loss, Accuracy) on Training Dataset (2.3022, 0.10)\n",
      "(Loss, Accuracy) on Validataion Dataset (2.3017, 0.11)\n"
     ]
    }
   ],
   "source": [
    "# Clear old variables\n",
    "tf.reset_default_graph()    \n",
    "\n",
    "# Declare out simple model\n",
    "model = inception_model()    \n",
    "    \n",
    "# Now, create a tf.Session and train the model\n",
    "with tf.Session(config=conf) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    run_model(sess, model, X_train, Y_train, is_training=True, epochs=5)\n",
    "    print(\"(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model(sess, model, X_train, Y_train))\n",
    "    print(\"(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jb9dL-_fvN-K"
   },
   "source": [
    "## <a name=\"2\"></a>2. Design a better model on CIFAR-10\n",
    "\n",
    "Now it's your job to experiment with CNNs to train a model that achieves **<font color=red>>= 70% accuracy on the validation set</font>** of CIFAR-10. <br> You can reuse the implemented functions from above.\n",
    "\n",
    "### Things you can try to change:\n",
    "- Filter size\n",
    "- Number of filters\n",
    "- Pooling vs Strided Convolution\n",
    "- Network architectures\n",
    "- Optimizers\n",
    "- Activation functions\n",
    "- Regularizations\n",
    "- Model ensembles\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDwOqUMcEzt2"
   },
   "outputs": [],
   "source": [
    "def backprop(layer,y_true,y_true_cls, learning_rate=1e-4):\n",
    "        layer_flat, num_features= flatten_layer(layer)\n",
    "        \n",
    "        layer_fc=new_fc_layer(input=layer_flat,\n",
    "                             num_inputs=num_features,\n",
    "                             num_outputs=10,\n",
    "                             use_relu=True)\n",
    "        \n",
    "#         y_pred=tf.nn.softmax(layer_fc)\n",
    "        \n",
    "#         y_pred_cls=tf.argmax(y_pred,axis=1)\n",
    "        \n",
    "        \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc,\n",
    "                                                        labels=y_true)\n",
    "        \n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "#         correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOYeznOF-_xe"
   },
   "outputs": [],
   "source": [
    "# Create a function to train and evaluata a model\n",
    "# You can reuse this function throughout the assignment\n",
    "def my_run_model(session, model, X, Y, epochs=1, batch_size=500, is_training=False):\n",
    "    # For training the model\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if is_training:\n",
    "        for i in range(epochs):\n",
    "\n",
    "            idx=np.random.randint(X.shape[0],size=batch_size)\n",
    "            \n",
    "            \n",
    "            X_train_batch=X[idx,:]\n",
    "            \n",
    "            Y_train_batch=Y[idx]\n",
    "            \n",
    "            feed_dict_train = {model.Input_image: X_train_batch,\n",
    "                               \n",
    "                               model.y_true_cls: Y_train_batch}\n",
    "\n",
    "            session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    " # Ending time.\n",
    "        end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "        time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "        print(\"#Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "#         print(\"#Training done!\")\n",
    "\n",
    "    # Evaluate loss and accuracy of the model\n",
    "    else:\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "#         loss=model.cross_entropy\n",
    "        \n",
    "#         accuracy=model.correct_prediction\n",
    "        \n",
    "       \n",
    "\n",
    "        feed_dict = {model.Input_image: X,\n",
    "                       model.y_true_cls: Y}\n",
    "        \n",
    "        accuracy = session.run(model.accuracy, feed_dict = feed_dict)\n",
    "         \n",
    "        loss = session.run(model.cost, feed_dict = feed_dict)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         accuracy=int(accuracy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        '''\n",
    "        return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZnd5Bt1vN-M",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class my_model(object):\n",
    "    def __init__(self):\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        #################Try GoogleNet structure################################\n",
    "        \n",
    "        self.learning_rate=1e-2\n",
    "        \n",
    "        self.inception_number=1\n",
    "        \n",
    "        self.Input_image=tf.placeholder(tf.float32,shape=[None,32,32,3],name='Input_image')\n",
    "        \n",
    "        self.Input_image_norm = tf.contrib.layers.batch_norm(self.Input_image, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=True)\n",
    "        \n",
    "        self.y_true_cls=tf.placeholder(tf.int64,shape=[None,],name='y_true_cls')\n",
    "        \n",
    "        self.y_true=tf.one_hot(indices=self.y_true_cls,depth=10)\n",
    "        \n",
    "  \n",
    "        self.stem_layer1 , self.stem_weights1=new_conv_layer(input=self.Input_image_norm,\n",
    "                           num_input_channels=int(self.Input_image.shape[3]),\n",
    "                           filter_size=9,\n",
    "                           num_filters=8,\n",
    "                           use_pooling=True)\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "#         self.stem_layer1 = tf.nn.dropout(self.stem_layer1,keep_prob=0.7)\n",
    "        \n",
    "        self.stem_layer2 , self.stem_weights2=new_conv_layer(input=self.stem_layer1,\n",
    "                           num_input_channels=int(self.stem_layer1.shape[3]),\n",
    "                           filter_size=9,\n",
    "                           num_filters=8,\n",
    "                           use_pooling=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "#         self.stem_layer2 = tf.nn.dropout(self.stem_layer2,keep_prob=0.7)\n",
    "        \n",
    "        self.inception = Inception_module(self.stem_layer2,8,3,8,3,8,3)\n",
    "        \n",
    "        self.inception_arr=[]\n",
    "        \n",
    "        self.inception_arr=self.inception_arr+[self.inception]\n",
    "        for i in range(self.inception_number-1):\n",
    "            self.inception_arr=self.inception_arr+[Inception_module(self.inception_arr[i],8,3,8,3,8,3)]\n",
    "            self.inception_arr[i+1] = tf.contrib.layers.batch_norm(self.inception_arr[i+1], \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=True,\n",
    "                                          scope='bn')\n",
    "            \n",
    "            self.inception_arr[i+1] = tf.nn.relu(self.inception_arr[i+1],'relu')\n",
    "            '''\n",
    "        for i in range(self.inception_number-1):\n",
    "            self.inception=Inception_module(self.inception,8,3,8,3,8,3)\n",
    "            \n",
    "            self.inception = tf.contrib.layers.batch_norm(self.inception, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=True,\n",
    "                                          scope='bn')\n",
    "            \n",
    "            self.inception = tf.nn.relu(self.inception,'relu')\n",
    "            \n",
    "#             self.inception = tf.nn.dropout(self.inception,keep_prob=0.4)\n",
    "        '''\n",
    "        \n",
    "#         self.inception_dropout = tf.nn.dropout(self.inception_arr[self.inception_number-1],keep_prob=0.75)\n",
    "        \n",
    "        self.stem_layer_inception , self.stem_weights_inception=new_conv_layer(input=self.inception_arr[self.inception_number-1],\n",
    "                           num_input_channels=int(self.inception_arr[self.inception_number-1].shape[3]),\n",
    "                           filter_size=9,\n",
    "                           num_filters=8,\n",
    "                           use_pooling=True)\n",
    "        \n",
    "        self.layer_flat, self.num_features= flatten_layer(self.stem_layer_inception)\n",
    "        \n",
    "        self.layer_fc=new_fc_layer(input=self.layer_flat,\n",
    "                             num_inputs=self.num_features,\n",
    "                             num_outputs=10,\n",
    "                             use_relu=True)\n",
    "        \n",
    "        self.layer_fc = tf.contrib.layers.batch_norm(self.layer_fc, \n",
    "                                          center=True, scale=True, \n",
    "                                          is_training=True,\n",
    "                                          scope='bn')\n",
    "        \n",
    "#         self.layer_fc = tf.nn.dropout(self.layer_fc,keep_prob=0.5)\n",
    "        \n",
    "        self.layer_fc=tf.nn.relu(self.layer_fc, 'relu')\n",
    "        \n",
    "        self.y_pred=tf.nn.softmax(self.layer_fc)\n",
    "        \n",
    "        self.y_pred_cls=tf.argmax(self.y_pred,axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.layer_fc,\n",
    "                                                        labels=self.y_true)\n",
    "        \n",
    "        self.cost = tf.reduce_mean(self.cross_entropy)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        \n",
    "        self.correct_prediction = tf.equal(self.y_pred_cls, self.y_true_cls)\n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer_new(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of filters.\n",
    "                   conv_strides,\n",
    "                   pool_size,\n",
    "                   pool_strides,\n",
    "                   use_pooling=True):  # Use 3x3 max-pooling.\n",
    "##################--Whats New Here--#################################\n",
    "# 1. Can give conv_strides and pool_strides as input\n",
    "# 2. Can give already_trained weights as input\n",
    "####################################################################\n",
    "    \n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = new_weights(shape=shape)\n",
    "#     shape = [filter_size, filter_size, 1, num_filters]\n",
    "    \n",
    "\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, conv_strides, conv_strides, 1],\n",
    "                         padding='SAME')                           \n",
    "\n",
    "    if use_pooling:\n",
    "        \n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, pool_size, pool_size, 1],\n",
    "                               strides= [1, pool_strides, pool_strides, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train and evaluata a model\n",
    "# You can reuse this function throughout the assignment\n",
    "import copy\n",
    "def run_model_v2(session, model, X, Y, epochs=1, batch_size=500, is_training=False, learn_rate = \"slow\"):\n",
    "    # For training the model\n",
    "    start_time = time.time()\n",
    "    mode = 0\n",
    "    acc = 0\n",
    "    new_acc = 0\n",
    "    best_train_acc = 0\n",
    "    best_val_acc = 0\n",
    "    val_acc = 0\n",
    "    i_learning_rate = copy.deepcopy(model.learning_rate)\n",
    "    i_l2_constant = copy.deepcopy(model.l2_constant)\n",
    "    feed_dict_val_acc = {model.Input_image: X_val,\n",
    "                       model.y_true_cls: Y_val,\n",
    "                         model.is_training : False}\n",
    "    feed_dict_train_acc = {model.Input_image: X,\n",
    "                            model.y_true_cls: Y,\n",
    "                            model.is_training : False}\n",
    "        \n",
    "    if is_training:\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            idx=np.random.randint(X.shape[0],size=batch_size)\n",
    "        \n",
    "            \n",
    "            X_train_batch=X[idx,:]\n",
    "            \n",
    "            Y_train_batch=Y[idx]\n",
    "            \n",
    "            feed_dict_train = {model.Input_image: X_train_batch,\n",
    "                               model.y_true_cls: Y_train_batch,\n",
    "                               model.is_training : True}\n",
    "            \n",
    "            if(i%100 == 0):\n",
    "                new_acc = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                new_acc = np.round(new_acc, 4)\n",
    "                if(new_acc>best_train_acc):\n",
    "                    best_train_acc = new_acc\n",
    "                    val_acc = session.run(model.accuracy, feed_dict = feed_dict_val_acc)\n",
    "                    val_acc = np.round(val_acc, 4)\n",
    "                    if(val_acc > best_val_acc):\n",
    "                        best_val_acc = val_acc\n",
    "                    print(\"#\",i,\" accuracy: \",new_acc, \" improvement: \", np.round(new_acc - acc, 4), \"best train accuracy:\",best_train_acc, \"validation accuracy: \", val_acc, best_val_acc)\n",
    "                else:\n",
    "                    print(\"iteration:\",i,\" accuracy: \",new_acc, \" improvement: \", np.round(new_acc - acc, 4))\n",
    "                acc = new_acc\n",
    "            model.learning_rate = i_learning_rate*np.random.randint(100)/50\n",
    "            model.l2_constant = i_l2_constant*np.random.randint(100)/50\n",
    "            \n",
    "            if(mode == 0):      \n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/50) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.4):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 0 to 1 at iteration: %d\",i)\n",
    "            if (mode == 1):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/50) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.45):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 1 to 2 at iteration: %d\",i)                \n",
    "            if(mode == 2):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/50) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.50):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 2 to 3 at iteration: %d\",i)\n",
    "            if (mode == 3):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/50) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.55):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 3 to 4 at iteration: %d\",i)     \n",
    "            if(mode == 4):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/50) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.60):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 4 to 5 at iteration: %d\",i)\n",
    "            if (mode == 5):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/100) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.65):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 5 to 6 at iteration: %d\",i)           \n",
    "            if(mode == 6):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/100) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.70):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 3\n",
    "                        print(\"mode 6 to 7 at iteration: %d\",i)\n",
    "            if (mode == 7):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/100) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.75):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 100\n",
    "                        print(\"mode 7 to 8 at iteration: %d\",i)\n",
    "            if (mode == 8):\n",
    "                session.run(model.optimizer, feed_dict=feed_dict_train)\n",
    "                if (i%(epochs/100) == 0):\n",
    "                    accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "                    if(accuracy >= 0.80):\n",
    "                        mode = mode + 1\n",
    "                        i_learning_rate /= 100\n",
    "                        print(\"mode 8 to 9 at iteration: %d\",i)\n",
    "            if (mode == 9):\n",
    "                session.run(model.optimizer, feed_dict = feed_dict_train)\n",
    "\n",
    "     # Ending time.\n",
    "        end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "        time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "        print(\"#Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "        print(\"#Training done!\")\n",
    "\n",
    "    # Evaluate loss and accuracy of the model\n",
    "    else:\n",
    "        accuracy = session.run(model.accuracy, feed_dict = feed_dict_train_acc)\n",
    "         \n",
    "        loss = session.run(model.cost, feed_dict = feed_dict_train_acc)\n",
    "\n",
    "        return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5khqF01TvN-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstart_time=time.time()\\n\\n# Clear old variables\\ntf.reset_default_graph()    \\n\\n# Declare out simple model\\n\\nmodel = my_model()\\nmodel.learning_rate=1e-10\\nmodel.inception_number=1\\n# Now, create a tf.Session and train the model\\nwith tf.Session(config=conf) as sess:\\n    ##############################################################################\\n    #                          IMPLEMENT YOUR CODE                               #\\n    ##############################################################################\\n    \\n    sess.run(tf.global_variables_initializer())\\n    \\n    my_run_model(sess, model, X_train, Y_train, batch_size=500, is_training=True, epochs=1000)\\n    \\n    print(my_run_model(sess,model,X_train, Y_train, is_training=False))\\n    print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\\n    \\n#     model.learning_rate=1e-4\\n    \\n    mode=0\\n    for i in range(2,8):\\n      model.inception_number=i\\n#       model.learning_rate=model.learning_rate/10\\n      \\n#       self.learning_rate*=10\\n      \\n      if(mode==0 and my_run_model(sess,model,X_train, Y_train, is_training=False)[1]>0.5):\\n            print(\"mode 0 to 1\")\\n#             model.learning_rate*=10\\n            mode=mode+1\\n      if(mode==1 and my_run_model(sess,model,X_train, Y_train, is_training=False)[1]>0.6):\\n            print(\"mode 1 to 2\")\\n#             model.learning_rate*=10\\n            mode=mode+1\\n      if(mode==2 and my_run_model(sess,model,X_train, Y_train, is_training=False)[1]>0.7):\\n            print(\"mode 2 to 3\")\\n            model.learning_rate*=10\\n            mode=mode+1\\n      my_run_model(sess,model,X_train, Y_train, batch_size=500, is_training=True, epochs=1000)\\n      print(my_run_model(sess,model,X_train, Y_train, is_training=False))\\n      print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\\n    \\n\\n    ##############################################################################\\n    #                             END OF YOUR CODE                               #\\n    ##############################################################################\\n    print(\"#(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model(sess, model, X_train, Y_train))\\n    print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\\n    \\n    end_time=time.time()\\n    time_dif = end_time - start_time\\n\\n   \\n    print(\"#total Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\\n    #Save your final model\\n    saver = tf.train.Saver()\\n    saver.save(sess, \"./model_checkpoints/my_model_final\")\\n    '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "'''\n",
    "start_time=time.time()\n",
    "\n",
    "# Clear old variables\n",
    "tf.reset_default_graph()    \n",
    "\n",
    "# Declare out simple model\n",
    "\n",
    "model = my_model()\n",
    "model.learning_rate=1e-10\n",
    "model.inception_number=1\n",
    "# Now, create a tf.Session and train the model\n",
    "with tf.Session(config=conf) as sess:\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    my_run_model(sess, model, X_train, Y_train, batch_size=500, is_training=True, epochs=1000)\n",
    "    \n",
    "    print(my_run_model(sess,model,X_train, Y_train, is_training=False))\n",
    "    print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\n",
    "    \n",
    "#     model.learning_rate=1e-4\n",
    "    \n",
    "    mode=0\n",
    "    for i in range(2,8):\n",
    "      model.inception_number=i\n",
    "#       model.learning_rate=model.learning_rate/10\n",
    "      \n",
    "#       self.learning_rate*=10\n",
    "      \n",
    "      if(mode==0 and my_run_model(sess,model,X_train, Y_train, is_training=False)[1]>0.5):\n",
    "            print(\"mode 0 to 1\")\n",
    "#             model.learning_rate*=10\n",
    "            mode=mode+1\n",
    "      if(mode==1 and my_run_model(sess,model,X_train, Y_train, is_training=False)[1]>0.6):\n",
    "            print(\"mode 1 to 2\")\n",
    "#             model.learning_rate*=10\n",
    "            mode=mode+1\n",
    "      if(mode==2 and my_run_model(sess,model,X_train, Y_train, is_training=False)[1]>0.7):\n",
    "            print(\"mode 2 to 3\")\n",
    "            model.learning_rate*=10\n",
    "            mode=mode+1\n",
    "      my_run_model(sess,model,X_train, Y_train, batch_size=500, is_training=True, epochs=1000)\n",
    "      print(my_run_model(sess,model,X_train, Y_train, is_training=False))\n",
    "      print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\n",
    "    \n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    print(\"#(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model(sess, model, X_train, Y_train))\n",
    "    print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model(sess, model, X_val, Y_val))\n",
    "    \n",
    "    end_time=time.time()\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "   \n",
    "    print(\"#total Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    #Save your final model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"./model_checkpoints/my_model_final\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model_simple1(object):\n",
    "    def __init__(self):\n",
    "        ##############################################################################\n",
    "        #                          IMPLEMENT YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        \n",
    "        #################Try GoogleNet structure################################\n",
    "        \n",
    "        #The input layer\n",
    "        self.Input_image=tf.placeholder(tf.float32,shape=[None,32,32,3],name='Input_image')\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "        ###################The labels#########################\n",
    "        self.y_true_cls=tf.placeholder(tf.int64,shape=[None,],name='y_true_cls')\n",
    "        self.y_true=tf.one_hot(indices=self.y_true_cls,depth=10)\n",
    "        ######################################################\n",
    "        self.learning_rate = 5e-4\n",
    "        #The first stem layer\n",
    "        \n",
    "        ############ data augmentation ##########################\n",
    "        rand_angle=random.uniform(0,360)\n",
    "        rand_int=random.randrange(0,4) # random number for rotation and flip\n",
    "        \n",
    "        self.Input_image_aug=tf.contrib.image.rotate(images=self.Input_image,angles=rand_angle)\n",
    "       \n",
    "        if(rand_int==1):\n",
    "            self.Input_image_aug=tf.image.random_flip_left_right(self.Input_image_aug)\n",
    "        elif(rand_int==2):\n",
    "            self.Input_image_aug=tf.image.random_flip_up_down(self.Input_image_aug)\n",
    "        elif(rand_int==3):\n",
    "            self.Input_image_aug=tf.image.random_flip_left_right(self.Input_image_aug)\n",
    "            self.Input_image_aug=tf.image.random_flip_up_down(self.Input_image_aug)\n",
    "        \n",
    "        self.Input_image_aug=tf.contrib.image.translate(images=self.Input_image_aug,\n",
    "                                                       translations=[random.randrange(-14,14),\n",
    "                                                                     random.randrange(-14,14)])\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ##########################################################\n",
    "        self.stem_layer1 , self.stem_weights1=new_conv_layer_new(input=self.Input_image,\n",
    "                                num_input_channels=int(self.Input_image.shape[3]),\n",
    "                                filter_size=4,\n",
    "                                num_filters=32,\n",
    "                                conv_strides = 2,\n",
    "                                pool_size = 3,\n",
    "                                pool_strides = 2,\n",
    "                                use_pooling=True)\n",
    "        self.stem_layer1 = tf.layers.batch_normalization(self.stem_layer1, training = self.is_training)\n",
    "        #The first inception layer\n",
    "        self.inception1, self.iweight1 = Inception_module2(self.stem_layer1,8,12,16,2,4,4)\n",
    "        \n",
    "        #The second inception layer\n",
    "        self.inception2, self.iweight2 = Inception_module2(self.inception1,16,16,24,4,12,8)\n",
    "        \n",
    "        #################The output layer#######################\n",
    "        self.inception2 = tf.nn.dropout(self.inception2,\n",
    "                                        keep_prob=0.6\n",
    "                                    )\n",
    "        self.layer_flat_out, self.num_features_out= flatten_layer(self.inception2)                \n",
    "        self.layer_fc_out=new_fc_layer(input=self.layer_flat_out,\n",
    "                             num_inputs=self.num_features_out,\n",
    "                             num_outputs=10,\n",
    "                             use_relu=True)\n",
    "        self.y_pred_out=tf.nn.softmax(self.layer_fc_out)\n",
    "        self.y_pred_cls_out=tf.argmax(self.y_pred_out,axis=1)\n",
    "        self.cross_entropy_out = tf.nn.softmax_cross_entropy_with_logits(logits=self.layer_fc_out,\n",
    "                                                        labels=self.y_true)\n",
    "        self.l2_constant = 1e-3\n",
    "        self.vars   = tf.trainable_variables() \n",
    "        self.lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in self.vars ]) * self.l2_constant\n",
    "        self.cost_out = tf.reduce_mean(self.cross_entropy_out+self.lossL2)\n",
    "        self.update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.cost_out)\n",
    "        ##################################################################\n",
    "        \n",
    "        #The cost\n",
    "        self.cost = self.cost_out\n",
    "        \n",
    "        #The accuracy\n",
    "        self.correct_prediction = tf.equal(self.y_pred_cls_out, self.y_true_cls)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 0  accuracy:  0.1089  improvement:  0.1089 best train accuracy: 0.1089 validation accuracy:  0.1078 0.1078\n",
      "# 100  accuracy:  0.325  improvement:  0.2161 best train accuracy: 0.325 validation accuracy:  0.3238 0.3238\n",
      "# 200  accuracy:  0.4128  improvement:  0.0878 best train accuracy: 0.4128 validation accuracy:  0.4103 0.4103\n",
      "# 300  accuracy:  0.447  improvement:  0.0342 best train accuracy: 0.447 validation accuracy:  0.4333 0.4333\n",
      "# 400  accuracy:  0.4763  improvement:  0.0293 best train accuracy: 0.4763 validation accuracy:  0.4718 0.4718\n",
      "# 500  accuracy:  0.4902  improvement:  0.0139 best train accuracy: 0.4902 validation accuracy:  0.487 0.487\n",
      "# 600  accuracy:  0.5109  improvement:  0.0207 best train accuracy: 0.5109 validation accuracy:  0.4994 0.4994\n",
      "# 700  accuracy:  0.5321  improvement:  0.0212 best train accuracy: 0.5321 validation accuracy:  0.5205 0.5205\n",
      "iteration: 800  accuracy:  0.5286  improvement:  -0.0035\n",
      "# 900  accuracy:  0.5431  improvement:  0.0145 best train accuracy: 0.5431 validation accuracy:  0.5314 0.5314\n",
      "# 1000  accuracy:  0.5442  improvement:  0.0011 best train accuracy: 0.5442 validation accuracy:  0.5377 0.5377\n",
      "# 1100  accuracy:  0.556  improvement:  0.0118 best train accuracy: 0.556 validation accuracy:  0.5447 0.5447\n",
      "# 1200  accuracy:  0.5737  improvement:  0.0177 best train accuracy: 0.5737 validation accuracy:  0.5578 0.5578\n",
      "iteration: 1300  accuracy:  0.5721  improvement:  -0.0016\n",
      "iteration: 1400  accuracy:  0.5731  improvement:  0.001\n",
      "# 1500  accuracy:  0.5828  improvement:  0.0097 best train accuracy: 0.5828 validation accuracy:  0.5703 0.5703\n",
      "# 1600  accuracy:  0.5876  improvement:  0.0048 best train accuracy: 0.5876 validation accuracy:  0.5797 0.5797\n",
      "iteration: 1700  accuracy:  0.586  improvement:  -0.0016\n",
      "# 1800  accuracy:  0.5998  improvement:  0.0138 best train accuracy: 0.5998 validation accuracy:  0.5791 0.5797\n",
      "# 1900  accuracy:  0.6032  improvement:  0.0034 best train accuracy: 0.6032 validation accuracy:  0.5871 0.5871\n",
      "# 2000  accuracy:  0.6096  improvement:  0.0064 best train accuracy: 0.6096 validation accuracy:  0.5924 0.5924\n",
      "mode 0 to 1 at iteration: %d 2000\n",
      "mode 1 to 2 at iteration: %d 2000\n",
      "mode 2 to 3 at iteration: %d 2000\n",
      "mode 3 to 4 at iteration: %d 2000\n",
      "iteration: 2100  accuracy:  0.606  improvement:  -0.0036\n",
      "# 2200  accuracy:  0.6232  improvement:  0.0172 best train accuracy: 0.6232 validation accuracy:  0.6068 0.6068\n",
      "iteration: 2300  accuracy:  0.613  improvement:  -0.0102\n",
      "# 2400  accuracy:  0.6239  improvement:  0.0109 best train accuracy: 0.6239 validation accuracy:  0.6032 0.6068\n",
      "# 2500  accuracy:  0.6322  improvement:  0.0083 best train accuracy: 0.6322 validation accuracy:  0.6085 0.6085\n",
      "# 2600  accuracy:  0.6372  improvement:  0.005 best train accuracy: 0.6372 validation accuracy:  0.6129 0.6129\n",
      "# 2700  accuracy:  0.6403  improvement:  0.0031 best train accuracy: 0.6403 validation accuracy:  0.6191 0.6191\n",
      "# 2800  accuracy:  0.6456  improvement:  0.0053 best train accuracy: 0.6456 validation accuracy:  0.6198 0.6198\n",
      "iteration: 2900  accuracy:  0.6448  improvement:  -0.0008\n",
      "# 3000  accuracy:  0.6536  improvement:  0.0088 best train accuracy: 0.6536 validation accuracy:  0.6333 0.6333\n",
      "# 3100  accuracy:  0.6566  improvement:  0.003 best train accuracy: 0.6566 validation accuracy:  0.6361 0.6361\n",
      "# 3200  accuracy:  0.6599  improvement:  0.0033 best train accuracy: 0.6599 validation accuracy:  0.632 0.6361\n",
      "iteration: 3300  accuracy:  0.6592  improvement:  -0.0007\n",
      "# 3400  accuracy:  0.6668  improvement:  0.0076 best train accuracy: 0.6668 validation accuracy:  0.6432 0.6432\n",
      "iteration: 3500  accuracy:  0.6618  improvement:  -0.005\n",
      "iteration: 3600  accuracy:  0.6624  improvement:  0.0006\n",
      "# 3700  accuracy:  0.6707  improvement:  0.0083 best train accuracy: 0.6707 validation accuracy:  0.6431 0.6432\n",
      "# 3800  accuracy:  0.6712  improvement:  0.0005 best train accuracy: 0.6712 validation accuracy:  0.6453 0.6453\n",
      "# 3900  accuracy:  0.6722  improvement:  0.001 best train accuracy: 0.6722 validation accuracy:  0.6425 0.6453\n",
      "# 4000  accuracy:  0.6769  improvement:  0.0047 best train accuracy: 0.6769 validation accuracy:  0.6471 0.6471\n",
      "mode 4 to 5 at iteration: %d 4000\n",
      "mode 5 to 6 at iteration: %d 4000\n",
      "iteration: 4100  accuracy:  0.6701  improvement:  -0.0068\n",
      "iteration: 4200  accuracy:  0.6747  improvement:  0.0046\n",
      "iteration: 4300  accuracy:  0.6744  improvement:  -0.0003\n",
      "# 4400  accuracy:  0.684  improvement:  0.0096 best train accuracy: 0.684 validation accuracy:  0.6552 0.6552\n",
      "# 4500  accuracy:  0.6861  improvement:  0.0021 best train accuracy: 0.6861 validation accuracy:  0.6549 0.6552\n",
      "iteration: 4600  accuracy:  0.6801  improvement:  -0.006\n",
      "# 4700  accuracy:  0.6864  improvement:  0.0063 best train accuracy: 0.6864 validation accuracy:  0.654 0.6552\n",
      "# 4800  accuracy:  0.6906  improvement:  0.0042 best train accuracy: 0.6906 validation accuracy:  0.6581 0.6581\n",
      "# 4900  accuracy:  0.699  improvement:  0.0084 best train accuracy: 0.699 validation accuracy:  0.666 0.666\n",
      "# 5000  accuracy:  0.7036  improvement:  0.0046 best train accuracy: 0.7036 validation accuracy:  0.6675 0.6675\n",
      "mode 6 to 7 at iteration: %d 5000\n",
      "iteration: 5100  accuracy:  0.6973  improvement:  -0.0063\n",
      "iteration: 5200  accuracy:  0.7035  improvement:  0.0062\n",
      "# 5300  accuracy:  0.7046  improvement:  0.0011 best train accuracy: 0.7046 validation accuracy:  0.6663 0.6675\n",
      "iteration: 5400  accuracy:  0.7036  improvement:  -0.001\n",
      "iteration: 5500  accuracy:  0.7036  improvement:  0.0\n",
      "iteration: 5600  accuracy:  0.7039  improvement:  0.0003\n",
      "iteration: 5700  accuracy:  0.7024  improvement:  -0.0015\n",
      "# 5800  accuracy:  0.7057  improvement:  0.0033 best train accuracy: 0.7057 validation accuracy:  0.6701 0.6701\n",
      "# 5900  accuracy:  0.7084  improvement:  0.0027 best train accuracy: 0.7084 validation accuracy:  0.6748 0.6748\n",
      "# 6000  accuracy:  0.7126  improvement:  0.0042 best train accuracy: 0.7126 validation accuracy:  0.6772 0.6772\n",
      "# 6100  accuracy:  0.7155  improvement:  0.0029 best train accuracy: 0.7155 validation accuracy:  0.6731 0.6772\n",
      "iteration: 6200  accuracy:  0.7128  improvement:  -0.0027\n",
      "iteration: 6300  accuracy:  0.7122  improvement:  -0.0006\n",
      "iteration: 6400  accuracy:  0.7152  improvement:  0.003\n",
      "iteration: 6500  accuracy:  0.7152  improvement:  0.0\n",
      "# 6600  accuracy:  0.7222  improvement:  0.007 best train accuracy: 0.7222 validation accuracy:  0.6775 0.6775\n",
      "iteration: 6700  accuracy:  0.7149  improvement:  -0.0073\n",
      "iteration: 6800  accuracy:  0.7156  improvement:  0.0007\n",
      "iteration: 6900  accuracy:  0.7217  improvement:  0.0061\n",
      "iteration: 7000  accuracy:  0.7154  improvement:  -0.0063\n",
      "iteration: 7100  accuracy:  0.7186  improvement:  0.0032\n",
      "iteration: 7200  accuracy:  0.7186  improvement:  0.0\n",
      "# 7300  accuracy:  0.7283  improvement:  0.0097 best train accuracy: 0.7283 validation accuracy:  0.6878 0.6878\n",
      "iteration: 7400  accuracy:  0.7265  improvement:  -0.0018\n",
      "iteration: 7500  accuracy:  0.7248  improvement:  -0.0017\n",
      "iteration: 7600  accuracy:  0.7242  improvement:  -0.0006\n",
      "iteration: 7700  accuracy:  0.7266  improvement:  0.0024\n",
      "iteration: 7800  accuracy:  0.722  improvement:  -0.0046\n",
      "iteration: 7900  accuracy:  0.726  improvement:  0.004\n",
      "# 8000  accuracy:  0.7366  improvement:  0.0106 best train accuracy: 0.7366 validation accuracy:  0.6963 0.6963\n",
      "iteration: 8100  accuracy:  0.7259  improvement:  -0.0107\n",
      "iteration: 8200  accuracy:  0.7265  improvement:  0.0006\n",
      "# 8300  accuracy:  0.7394  improvement:  0.0129 best train accuracy: 0.7394 validation accuracy:  0.6933 0.6963\n",
      "iteration: 8400  accuracy:  0.7316  improvement:  -0.0078\n",
      "iteration: 8500  accuracy:  0.7391  improvement:  0.0075\n",
      "iteration: 8600  accuracy:  0.7391  improvement:  0.0\n",
      "iteration: 8700  accuracy:  0.7365  improvement:  -0.0026\n",
      "iteration: 8800  accuracy:  0.7332  improvement:  -0.0033\n",
      "iteration: 8900  accuracy:  0.7389  improvement:  0.0057\n",
      "iteration: 9000  accuracy:  0.7382  improvement:  -0.0007\n",
      "# 9100  accuracy:  0.7441  improvement:  0.0059 best train accuracy: 0.7441 validation accuracy:  0.6941 0.6963\n",
      "iteration: 9200  accuracy:  0.7351  improvement:  -0.009\n",
      "iteration: 9300  accuracy:  0.7401  improvement:  0.005\n",
      "iteration: 9400  accuracy:  0.7398  improvement:  -0.0003\n",
      "iteration: 9500  accuracy:  0.7395  improvement:  -0.0003\n",
      "iteration: 9600  accuracy:  0.7401  improvement:  0.0006\n",
      "iteration: 9700  accuracy:  0.7398  improvement:  -0.0003\n",
      "iteration: 9800  accuracy:  0.7417  improvement:  0.0019\n",
      "# 9900  accuracy:  0.7472  improvement:  0.0055 best train accuracy: 0.7472 validation accuracy:  0.7004 0.7004\n",
      "iteration: 10000  accuracy:  0.7386  improvement:  -0.0086\n",
      "# 10100  accuracy:  0.7546  improvement:  0.016 best train accuracy: 0.7546 validation accuracy:  0.6991 0.7004\n",
      "iteration: 10200  accuracy:  0.746  improvement:  -0.0086\n",
      "iteration: 10300  accuracy:  0.7445  improvement:  -0.0015\n",
      "iteration: 10400  accuracy:  0.7478  improvement:  0.0033\n",
      "iteration: 10500  accuracy:  0.7469  improvement:  -0.0009\n",
      "iteration: 10600  accuracy:  0.7484  improvement:  0.0015\n",
      "# 10700  accuracy:  0.7548  improvement:  0.0064 best train accuracy: 0.7548 validation accuracy:  0.7037 0.7037\n",
      "iteration: 10800  accuracy:  0.7475  improvement:  -0.0073\n",
      "iteration: 10900  accuracy:  0.7536  improvement:  0.0061\n",
      "iteration: 11000  accuracy:  0.7524  improvement:  -0.0012\n",
      "mode 7 to 8 at iteration: %d 11000\n",
      "iteration: 11100  accuracy:  0.7513  improvement:  -0.0011\n",
      "iteration: 11200  accuracy:  0.7546  improvement:  0.0033\n",
      "# 11300  accuracy:  0.763  improvement:  0.0084 best train accuracy: 0.763 validation accuracy:  0.7088 0.7088\n",
      "iteration: 11400  accuracy:  0.7552  improvement:  -0.0078\n",
      "iteration: 11500  accuracy:  0.7491  improvement:  -0.0061\n",
      "iteration: 11600  accuracy:  0.7598  improvement:  0.0107\n",
      "iteration: 11700  accuracy:  0.7565  improvement:  -0.0033\n",
      "iteration: 11800  accuracy:  0.7572  improvement:  0.0007\n",
      "iteration: 11900  accuracy:  0.7466  improvement:  -0.0106\n",
      "iteration: 12000  accuracy:  0.7628  improvement:  0.0162\n",
      "iteration: 12100  accuracy:  0.7536  improvement:  -0.0092\n",
      "iteration: 12200  accuracy:  0.7524  improvement:  -0.0012\n",
      "iteration: 12300  accuracy:  0.7603  improvement:  0.0079\n",
      "# 12400  accuracy:  0.7635  improvement:  0.0032 best train accuracy: 0.7635 validation accuracy:  0.7119 0.7119\n",
      "iteration: 12500  accuracy:  0.7619  improvement:  -0.0016\n",
      "iteration: 12600  accuracy:  0.7593  improvement:  -0.0026\n",
      "iteration: 12700  accuracy:  0.7594  improvement:  1e-04\n",
      "iteration: 12800  accuracy:  0.7597  improvement:  0.0003\n",
      "iteration: 12900  accuracy:  0.7603  improvement:  0.0006\n",
      "iteration: 13000  accuracy:  0.7629  improvement:  0.0026\n",
      "iteration: 13100  accuracy:  0.7536  improvement:  -0.0093\n",
      "# 13200  accuracy:  0.7658  improvement:  0.0122 best train accuracy: 0.7658 validation accuracy:  0.7109 0.7119\n",
      "iteration: 13300  accuracy:  0.7649  improvement:  -0.0009\n",
      "# 13400  accuracy:  0.766  improvement:  0.0011 best train accuracy: 0.766 validation accuracy:  0.7068 0.7119\n",
      "iteration: 13500  accuracy:  0.7612  improvement:  -0.0048\n",
      "# 13600  accuracy:  0.7705  improvement:  0.0093 best train accuracy: 0.7705 validation accuracy:  0.7096 0.7119\n",
      "iteration: 13700  accuracy:  0.7676  improvement:  -0.0029\n",
      "# 13800  accuracy:  0.7717  improvement:  0.0041 best train accuracy: 0.7717 validation accuracy:  0.7113 0.7119\n",
      "iteration: 13900  accuracy:  0.7682  improvement:  -0.0035\n",
      "iteration: 14000  accuracy:  0.7642  improvement:  -0.004\n",
      "iteration: 14100  accuracy:  0.7696  improvement:  0.0054\n",
      "iteration: 14200  accuracy:  0.7667  improvement:  -0.0029\n",
      "# 14300  accuracy:  0.7776  improvement:  0.0109 best train accuracy: 0.7776 validation accuracy:  0.7154 0.7154\n",
      "iteration: 14400  accuracy:  0.7637  improvement:  -0.0139\n",
      "iteration: 14500  accuracy:  0.7723  improvement:  0.0086\n",
      "iteration: 14600  accuracy:  0.7733  improvement:  0.001\n",
      "iteration: 14700  accuracy:  0.7776  improvement:  0.0043\n",
      "iteration: 14800  accuracy:  0.7675  improvement:  -0.0101\n",
      "iteration: 14900  accuracy:  0.766  improvement:  -0.0015\n",
      "iteration: 15000  accuracy:  0.7718  improvement:  0.0058\n",
      "iteration: 15100  accuracy:  0.7742  improvement:  0.0024\n",
      "iteration: 15200  accuracy:  0.777  improvement:  0.0028\n",
      "iteration: 15300  accuracy:  0.7688  improvement:  -0.0082\n",
      "# 15400  accuracy:  0.7798  improvement:  0.011 best train accuracy: 0.7798 validation accuracy:  0.7142 0.7154\n",
      "iteration: 15500  accuracy:  0.7772  improvement:  -0.0026\n",
      "iteration: 15600  accuracy:  0.7781  improvement:  0.0009\n",
      "iteration: 15700  accuracy:  0.7652  improvement:  -0.0129\n",
      "iteration: 15800  accuracy:  0.7772  improvement:  0.012\n",
      "iteration: 15900  accuracy:  0.776  improvement:  -0.0012\n",
      "iteration: 16000  accuracy:  0.774  improvement:  -0.002\n",
      "# 16100  accuracy:  0.78  improvement:  0.006 best train accuracy: 0.78 validation accuracy:  0.7149 0.7154\n",
      "iteration: 16200  accuracy:  0.779  improvement:  -0.001\n",
      "iteration: 16300  accuracy:  0.776  improvement:  -0.003\n",
      "iteration: 16400  accuracy:  0.7778  improvement:  0.0018\n",
      "iteration: 16500  accuracy:  0.7735  improvement:  -0.0043\n",
      "iteration: 16600  accuracy:  0.77  improvement:  -0.0035\n",
      "# 16700  accuracy:  0.7842  improvement:  0.0142 best train accuracy: 0.7842 validation accuracy:  0.716 0.716\n",
      "iteration: 16800  accuracy:  0.7778  improvement:  -0.0064\n",
      "iteration: 16900  accuracy:  0.779  improvement:  0.0012\n",
      "# 17000  accuracy:  0.7848  improvement:  0.0058 best train accuracy: 0.7848 validation accuracy:  0.719 0.719\n",
      "iteration: 17100  accuracy:  0.7827  improvement:  -0.0021\n",
      "iteration: 17200  accuracy:  0.7755  improvement:  -0.0072\n",
      "# 17300  accuracy:  0.7881  improvement:  0.0126 best train accuracy: 0.7881 validation accuracy:  0.7173 0.719\n",
      "iteration: 17400  accuracy:  0.7786  improvement:  -0.0095\n",
      "iteration: 17500  accuracy:  0.7851  improvement:  0.0065\n",
      "iteration: 17600  accuracy:  0.7855  improvement:  0.0004\n",
      "iteration: 17700  accuracy:  0.778  improvement:  -0.0075\n",
      "iteration: 17800  accuracy:  0.7839  improvement:  0.0059\n",
      "iteration: 17900  accuracy:  0.7839  improvement:  0.0\n",
      "iteration: 18000  accuracy:  0.7878  improvement:  0.0039\n",
      "iteration: 18100  accuracy:  0.7774  improvement:  -0.0104\n",
      "iteration: 18200  accuracy:  0.7666  improvement:  -0.0108\n",
      "iteration: 18300  accuracy:  0.7851  improvement:  0.0185\n",
      "iteration: 18400  accuracy:  0.7852  improvement:  1e-04\n",
      "iteration: 18500  accuracy:  0.7809  improvement:  -0.0043\n",
      "iteration: 18600  accuracy:  0.7828  improvement:  0.0019\n",
      "iteration: 18700  accuracy:  0.7846  improvement:  0.0018\n",
      "iteration: 18800  accuracy:  0.7772  improvement:  -0.0074\n",
      "iteration: 18900  accuracy:  0.7839  improvement:  0.0067\n",
      "iteration: 19000  accuracy:  0.7798  improvement:  -0.0041\n",
      "iteration: 19100  accuracy:  0.7798  improvement:  0.0\n",
      "iteration: 19200  accuracy:  0.785  improvement:  0.0052\n",
      "# 19300  accuracy:  0.7891  improvement:  0.0041 best train accuracy: 0.7891 validation accuracy:  0.7141 0.719\n",
      "iteration: 19400  accuracy:  0.7878  improvement:  -0.0013\n",
      "# 19500  accuracy:  0.791  improvement:  0.0032 best train accuracy: 0.791 validation accuracy:  0.7156 0.719\n",
      "iteration: 19600  accuracy:  0.7878  improvement:  -0.0032\n",
      "iteration: 19700  accuracy:  0.791  improvement:  0.0032\n",
      "iteration: 19800  accuracy:  0.7891  improvement:  -0.0019\n",
      "iteration: 19900  accuracy:  0.7869  improvement:  -0.0022\n",
      "# 20000  accuracy:  0.792  improvement:  0.0051 best train accuracy: 0.792 validation accuracy:  0.7175 0.719\n",
      "iteration: 20100  accuracy:  0.7874  improvement:  -0.0046\n",
      "iteration: 20200  accuracy:  0.7904  improvement:  0.003\n",
      "iteration: 20300  accuracy:  0.7844  improvement:  -0.006\n",
      "# 20400  accuracy:  0.7926  improvement:  0.0082 best train accuracy: 0.7926 validation accuracy:  0.7149 0.719\n",
      "iteration: 20500  accuracy:  0.7909  improvement:  -0.0017\n",
      "iteration: 20600  accuracy:  0.7834  improvement:  -0.0075\n",
      "# 20700  accuracy:  0.7958  improvement:  0.0124 best train accuracy: 0.7958 validation accuracy:  0.7195 0.7195\n",
      "iteration: 20800  accuracy:  0.7912  improvement:  -0.0046\n",
      "iteration: 20900  accuracy:  0.7872  improvement:  -0.004\n",
      "iteration: 21000  accuracy:  0.7909  improvement:  0.0037\n",
      "iteration: 21100  accuracy:  0.7902  improvement:  -0.0007\n",
      "iteration: 21200  accuracy:  0.7895  improvement:  -0.0007\n",
      "iteration: 21300  accuracy:  0.7937  improvement:  0.0042\n",
      "iteration: 21400  accuracy:  0.7942  improvement:  0.0005\n",
      "iteration: 21500  accuracy:  0.7879  improvement:  -0.0063\n",
      "iteration: 21600  accuracy:  0.7902  improvement:  0.0023\n",
      "iteration: 21700  accuracy:  0.7858  improvement:  -0.0044\n",
      "iteration: 21800  accuracy:  0.7928  improvement:  0.007\n",
      "# 21900  accuracy:  0.7996  improvement:  0.0068 best train accuracy: 0.7996 validation accuracy:  0.7204 0.7204\n",
      "iteration: 22000  accuracy:  0.7872  improvement:  -0.0124\n",
      "iteration: 22100  accuracy:  0.7949  improvement:  0.0077\n",
      "iteration: 22200  accuracy:  0.7959  improvement:  0.001\n",
      "iteration: 22300  accuracy:  0.7926  improvement:  -0.0033\n",
      "iteration: 22400  accuracy:  0.7892  improvement:  -0.0034\n",
      "iteration: 22500  accuracy:  0.7975  improvement:  0.0083\n",
      "iteration: 22600  accuracy:  0.7884  improvement:  -0.0091\n",
      "iteration: 22700  accuracy:  0.7962  improvement:  0.0078\n",
      "# 22800  accuracy:  0.8  improvement:  0.0038 best train accuracy: 0.8 validation accuracy:  0.7198 0.7204\n",
      "iteration: 22900  accuracy:  0.7988  improvement:  -0.0012\n",
      "iteration: 23000  accuracy:  0.7891  improvement:  -0.0097\n",
      "iteration: 23100  accuracy:  0.7901  improvement:  0.001\n",
      "iteration: 23200  accuracy:  0.7939  improvement:  0.0038\n",
      "iteration: 23300  accuracy:  0.7948  improvement:  0.0009\n",
      "iteration: 23400  accuracy:  0.7865  improvement:  -0.0083\n",
      "iteration: 23500  accuracy:  0.7984  improvement:  0.0119\n",
      "iteration: 23600  accuracy:  0.7893  improvement:  -0.0091\n",
      "iteration: 23700  accuracy:  0.7927  improvement:  0.0034\n",
      "iteration: 23800  accuracy:  0.7969  improvement:  0.0042\n",
      "# 23900  accuracy:  0.8004  improvement:  0.0035 best train accuracy: 0.8004 validation accuracy:  0.7162 0.7204\n",
      "iteration: 24000  accuracy:  0.797  improvement:  -0.0034\n",
      "iteration: 24100  accuracy:  0.792  improvement:  -0.005\n",
      "iteration: 24200  accuracy:  0.7987  improvement:  0.0067\n",
      "# 24300  accuracy:  0.803  improvement:  0.0043 best train accuracy: 0.803 validation accuracy:  0.7168 0.7204\n",
      "iteration: 24400  accuracy:  0.7983  improvement:  -0.0047\n",
      "iteration: 24500  accuracy:  0.7921  improvement:  -0.0062\n",
      "iteration: 24600  accuracy:  0.799  improvement:  0.0069\n",
      "iteration: 24700  accuracy:  0.7957  improvement:  -0.0033\n",
      "# 24800  accuracy:  0.8041  improvement:  0.0084 best train accuracy: 0.8041 validation accuracy:  0.7166 0.7204\n",
      "iteration: 24900  accuracy:  0.7986  improvement:  -0.0055\n",
      "iteration: 25000  accuracy:  0.7996  improvement:  0.001\n",
      "iteration: 25100  accuracy:  0.8026  improvement:  0.003\n",
      "iteration: 25200  accuracy:  0.8028  improvement:  0.0002\n",
      "iteration: 25300  accuracy:  0.8029  improvement:  1e-04\n",
      "iteration: 25400  accuracy:  0.7996  improvement:  -0.0033\n",
      "iteration: 25500  accuracy:  0.8005  improvement:  0.0009\n",
      "# 25600  accuracy:  0.8046  improvement:  0.0041 best train accuracy: 0.8046 validation accuracy:  0.7161 0.7204\n",
      "iteration: 25700  accuracy:  0.8045  improvement:  -1e-04\n",
      "iteration: 25800  accuracy:  0.8018  improvement:  -0.0027\n",
      "iteration: 25900  accuracy:  0.7976  improvement:  -0.0042\n",
      "iteration: 26000  accuracy:  0.8046  improvement:  0.007\n",
      "mode 8 to 9 at iteration: %d 26000\n",
      "# 26100  accuracy:  0.8055  improvement:  0.0009 best train accuracy: 0.8055 validation accuracy:  0.7206 0.7206\n",
      "iteration: 26200  accuracy:  0.7966  improvement:  -0.0089\n",
      "iteration: 26300  accuracy:  0.8012  improvement:  0.0046\n",
      "iteration: 26400  accuracy:  0.8048  improvement:  0.0036\n",
      "iteration: 26500  accuracy:  0.7962  improvement:  -0.0086\n",
      "iteration: 26600  accuracy:  0.8034  improvement:  0.0072\n",
      "iteration: 26700  accuracy:  0.802  improvement:  -0.0014\n",
      "# 26800  accuracy:  0.8114  improvement:  0.0094 best train accuracy: 0.8114 validation accuracy:  0.7252 0.7252\n",
      "iteration: 26900  accuracy:  0.8017  improvement:  -0.0097\n",
      "iteration: 27000  accuracy:  0.802  improvement:  0.0003\n",
      "iteration: 27100  accuracy:  0.8061  improvement:  0.0041\n",
      "iteration: 27200  accuracy:  0.7978  improvement:  -0.0083\n",
      "iteration: 27300  accuracy:  0.8075  improvement:  0.0097\n",
      "iteration: 27400  accuracy:  0.7996  improvement:  -0.0079\n",
      "iteration: 27500  accuracy:  0.8038  improvement:  0.0042\n",
      "iteration: 27600  accuracy:  0.8093  improvement:  0.0055\n",
      "iteration: 27700  accuracy:  0.8043  improvement:  -0.005\n",
      "iteration: 27800  accuracy:  0.799  improvement:  -0.0053\n",
      "iteration: 27900  accuracy:  0.8033  improvement:  0.0043\n",
      "iteration: 28000  accuracy:  0.8056  improvement:  0.0023\n",
      "iteration: 28100  accuracy:  0.8085  improvement:  0.0029\n",
      "iteration: 28200  accuracy:  0.8098  improvement:  0.0013\n",
      "iteration: 28300  accuracy:  0.7981  improvement:  -0.0117\n",
      "iteration: 28400  accuracy:  0.8053  improvement:  0.0072\n",
      "iteration: 28500  accuracy:  0.8016  improvement:  -0.0037\n",
      "iteration: 28600  accuracy:  0.8016  improvement:  0.0\n",
      "iteration: 28700  accuracy:  0.8023  improvement:  0.0007\n",
      "iteration: 28800  accuracy:  0.8026  improvement:  0.0003\n",
      "iteration: 28900  accuracy:  0.8049  improvement:  0.0023\n",
      "iteration: 29000  accuracy:  0.8038  improvement:  -0.0011\n",
      "iteration: 29100  accuracy:  0.8004  improvement:  -0.0034\n",
      "iteration: 29200  accuracy:  0.7991  improvement:  -0.0013\n",
      "iteration: 29300  accuracy:  0.8072  improvement:  0.0081\n",
      "iteration: 29400  accuracy:  0.8066  improvement:  -0.0006\n",
      "iteration: 29500  accuracy:  0.8068  improvement:  0.0002\n",
      "iteration: 29600  accuracy:  0.8094  improvement:  0.0026\n",
      "iteration: 29700  accuracy:  0.8098  improvement:  0.0004\n",
      "iteration: 29800  accuracy:  0.8097  improvement:  -1e-04\n",
      "iteration: 29900  accuracy:  0.8096  improvement:  -1e-04\n",
      "iteration: 30000  accuracy:  0.8097  improvement:  1e-04\n",
      "iteration: 30100  accuracy:  0.8071  improvement:  -0.0026\n",
      "iteration: 30200  accuracy:  0.8054  improvement:  -0.0017\n",
      "iteration: 30300  accuracy:  0.8097  improvement:  0.0043\n",
      "iteration: 30400  accuracy:  0.8024  improvement:  -0.0073\n",
      "iteration: 30500  accuracy:  0.8086  improvement:  0.0062\n",
      "iteration: 30600  accuracy:  0.8063  improvement:  -0.0023\n",
      "iteration: 30700  accuracy:  0.8074  improvement:  0.0011\n",
      "iteration: 30800  accuracy:  0.8076  improvement:  0.0002\n",
      "# 30900  accuracy:  0.8167  improvement:  0.0091 best train accuracy: 0.8167 validation accuracy:  0.7236 0.7252\n",
      "iteration: 31000  accuracy:  0.806  improvement:  -0.0107\n",
      "iteration: 31100  accuracy:  0.8024  improvement:  -0.0036\n",
      "iteration: 31200  accuracy:  0.8031  improvement:  0.0007\n",
      "iteration: 31300  accuracy:  0.8084  improvement:  0.0053\n",
      "iteration: 31400  accuracy:  0.8107  improvement:  0.0023\n",
      "iteration: 31500  accuracy:  0.8026  improvement:  -0.0081\n",
      "iteration: 31600  accuracy:  0.8114  improvement:  0.0088\n",
      "iteration: 31700  accuracy:  0.8067  improvement:  -0.0047\n",
      "iteration: 31800  accuracy:  0.8111  improvement:  0.0044\n",
      "iteration: 31900  accuracy:  0.8102  improvement:  -0.0009\n",
      "iteration: 32000  accuracy:  0.8113  improvement:  0.0011\n",
      "iteration: 32100  accuracy:  0.812  improvement:  0.0007\n",
      "iteration: 32200  accuracy:  0.8151  improvement:  0.0031\n",
      "iteration: 32300  accuracy:  0.8062  improvement:  -0.0089\n",
      "iteration: 32400  accuracy:  0.8123  improvement:  0.0061\n",
      "iteration: 32500  accuracy:  0.8115  improvement:  -0.0008\n",
      "iteration: 32600  accuracy:  0.8132  improvement:  0.0017\n",
      "iteration: 32700  accuracy:  0.8068  improvement:  -0.0064\n",
      "iteration: 32800  accuracy:  0.8163  improvement:  0.0095\n",
      "iteration: 32900  accuracy:  0.813  improvement:  -0.0033\n",
      "iteration: 33000  accuracy:  0.8102  improvement:  -0.0028\n",
      "iteration: 33100  accuracy:  0.808  improvement:  -0.0022\n",
      "iteration: 33200  accuracy:  0.8112  improvement:  0.0032\n",
      "iteration: 33300  accuracy:  0.8075  improvement:  -0.0037\n",
      "iteration: 33400  accuracy:  0.8145  improvement:  0.007\n",
      "iteration: 33500  accuracy:  0.8141  improvement:  -0.0004\n",
      "iteration: 33600  accuracy:  0.8129  improvement:  -0.0012\n",
      "iteration: 33700  accuracy:  0.8149  improvement:  0.002\n",
      "iteration: 33800  accuracy:  0.813  improvement:  -0.0019\n",
      "iteration: 33900  accuracy:  0.8104  improvement:  -0.0026\n",
      "iteration: 34000  accuracy:  0.8134  improvement:  0.003\n",
      "iteration: 34100  accuracy:  0.8106  improvement:  -0.0028\n",
      "iteration: 34200  accuracy:  0.8139  improvement:  0.0033\n",
      "iteration: 34300  accuracy:  0.8104  improvement:  -0.0035\n",
      "iteration: 34400  accuracy:  0.8139  improvement:  0.0035\n",
      "iteration: 34500  accuracy:  0.812  improvement:  -0.0019\n",
      "# 34600  accuracy:  0.8186  improvement:  0.0066 best train accuracy: 0.8186 validation accuracy:  0.7198 0.7252\n",
      "iteration: 34700  accuracy:  0.8114  improvement:  -0.0072\n",
      "iteration: 34800  accuracy:  0.8112  improvement:  -0.0002\n",
      "iteration: 34900  accuracy:  0.8146  improvement:  0.0034\n",
      "iteration: 35000  accuracy:  0.814  improvement:  -0.0006\n",
      "iteration: 35100  accuracy:  0.8062  improvement:  -0.0078\n",
      "iteration: 35200  accuracy:  0.8072  improvement:  0.001\n",
      "# 35300  accuracy:  0.82  improvement:  0.0128 best train accuracy: 0.82 validation accuracy:  0.7188 0.7252\n",
      "iteration: 35400  accuracy:  0.8089  improvement:  -0.0111\n",
      "iteration: 35500  accuracy:  0.8103  improvement:  0.0014\n",
      "iteration: 35600  accuracy:  0.813  improvement:  0.0027\n",
      "iteration: 35700  accuracy:  0.8128  improvement:  -0.0002\n",
      "iteration: 35800  accuracy:  0.8036  improvement:  -0.0092\n",
      "iteration: 35900  accuracy:  0.8082  improvement:  0.0046\n",
      "iteration: 36000  accuracy:  0.8146  improvement:  0.0064\n",
      "iteration: 36100  accuracy:  0.8086  improvement:  -0.006\n",
      "iteration: 36200  accuracy:  0.8159  improvement:  0.0073\n",
      "iteration: 36300  accuracy:  0.8056  improvement:  -0.0103\n",
      "iteration: 36400  accuracy:  0.8173  improvement:  0.0117\n",
      "iteration: 36500  accuracy:  0.8172  improvement:  -1e-04\n",
      "iteration: 36600  accuracy:  0.8152  improvement:  -0.002\n",
      "iteration: 36700  accuracy:  0.8147  improvement:  -0.0005\n",
      "iteration: 36800  accuracy:  0.8134  improvement:  -0.0013\n",
      "iteration: 36900  accuracy:  0.819  improvement:  0.0056\n",
      "iteration: 37000  accuracy:  0.8148  improvement:  -0.0042\n",
      "iteration: 37100  accuracy:  0.8181  improvement:  0.0033\n",
      "iteration: 37200  accuracy:  0.8114  improvement:  -0.0067\n",
      "iteration: 37300  accuracy:  0.8119  improvement:  0.0005\n",
      "iteration: 37400  accuracy:  0.8156  improvement:  0.0037\n",
      "iteration: 37500  accuracy:  0.8069  improvement:  -0.0087\n",
      "iteration: 37600  accuracy:  0.811  improvement:  0.0041\n",
      "iteration: 37700  accuracy:  0.8159  improvement:  0.0049\n",
      "# 37800  accuracy:  0.8228  improvement:  0.0069 best train accuracy: 0.8228 validation accuracy:  0.7278 0.7278\n",
      "iteration: 37900  accuracy:  0.8225  improvement:  -0.0003\n",
      "iteration: 38000  accuracy:  0.8205  improvement:  -0.002\n",
      "iteration: 38100  accuracy:  0.8182  improvement:  -0.0023\n",
      "iteration: 38200  accuracy:  0.8206  improvement:  0.0024\n",
      "iteration: 38300  accuracy:  0.8186  improvement:  -0.002\n",
      "iteration: 38400  accuracy:  0.8165  improvement:  -0.0021\n",
      "iteration: 38500  accuracy:  0.8194  improvement:  0.0029\n",
      "iteration: 38600  accuracy:  0.8131  improvement:  -0.0063\n",
      "iteration: 38700  accuracy:  0.8188  improvement:  0.0057\n",
      "iteration: 38800  accuracy:  0.8198  improvement:  0.001\n",
      "iteration: 38900  accuracy:  0.8164  improvement:  -0.0034\n",
      "iteration: 39000  accuracy:  0.8077  improvement:  -0.0087\n",
      "iteration: 39100  accuracy:  0.8202  improvement:  0.0125\n",
      "# 39200  accuracy:  0.8232  improvement:  0.003 best train accuracy: 0.8232 validation accuracy:  0.7192 0.7278\n",
      "iteration: 39300  accuracy:  0.8188  improvement:  -0.0044\n",
      "iteration: 39400  accuracy:  0.8166  improvement:  -0.0022\n",
      "iteration: 39500  accuracy:  0.8134  improvement:  -0.0032\n",
      "iteration: 39600  accuracy:  0.8137  improvement:  0.0003\n",
      "iteration: 39700  accuracy:  0.8153  improvement:  0.0016\n",
      "iteration: 39800  accuracy:  0.8222  improvement:  0.0069\n",
      "# 39900  accuracy:  0.8234  improvement:  0.0012 best train accuracy: 0.8234 validation accuracy:  0.7254 0.7278\n",
      "# 40000  accuracy:  0.825  improvement:  0.0016 best train accuracy: 0.825 validation accuracy:  0.7223 0.7278\n",
      "iteration: 40100  accuracy:  0.8232  improvement:  -0.0018\n",
      "iteration: 40200  accuracy:  0.8204  improvement:  -0.0028\n",
      "iteration: 40300  accuracy:  0.8205  improvement:  1e-04\n",
      "iteration: 40400  accuracy:  0.8112  improvement:  -0.0093\n",
      "iteration: 40500  accuracy:  0.82  improvement:  0.0088\n",
      "iteration: 40600  accuracy:  0.8206  improvement:  0.0006\n",
      "iteration: 40700  accuracy:  0.8198  improvement:  -0.0008\n",
      "iteration: 40800  accuracy:  0.8186  improvement:  -0.0012\n",
      "iteration: 40900  accuracy:  0.8207  improvement:  0.0021\n",
      "iteration: 41000  accuracy:  0.8205  improvement:  -0.0002\n",
      "iteration: 41100  accuracy:  0.8212  improvement:  0.0007\n",
      "iteration: 41200  accuracy:  0.814  improvement:  -0.0072\n",
      "iteration: 41300  accuracy:  0.8217  improvement:  0.0077\n",
      "# 41400  accuracy:  0.8268  improvement:  0.0051 best train accuracy: 0.8268 validation accuracy:  0.726 0.7278\n",
      "iteration: 41500  accuracy:  0.8209  improvement:  -0.0059\n",
      "iteration: 41600  accuracy:  0.82  improvement:  -0.0009\n",
      "iteration: 41700  accuracy:  0.8206  improvement:  0.0006\n",
      "iteration: 41800  accuracy:  0.8132  improvement:  -0.0074\n",
      "iteration: 41900  accuracy:  0.8181  improvement:  0.0049\n",
      "iteration: 42000  accuracy:  0.823  improvement:  0.0049\n",
      "iteration: 42100  accuracy:  0.8202  improvement:  -0.0028\n",
      "iteration: 42200  accuracy:  0.8254  improvement:  0.0052\n",
      "iteration: 42300  accuracy:  0.8156  improvement:  -0.0098\n",
      "iteration: 42400  accuracy:  0.8225  improvement:  0.0069\n",
      "iteration: 42500  accuracy:  0.8212  improvement:  -0.0013\n",
      "iteration: 42600  accuracy:  0.8174  improvement:  -0.0038\n",
      "iteration: 42700  accuracy:  0.8264  improvement:  0.009\n",
      "iteration: 42800  accuracy:  0.8243  improvement:  -0.0021\n",
      "iteration: 42900  accuracy:  0.822  improvement:  -0.0023\n",
      "iteration: 43000  accuracy:  0.8227  improvement:  0.0007\n",
      "iteration: 43100  accuracy:  0.816  improvement:  -0.0067\n",
      "iteration: 43200  accuracy:  0.823  improvement:  0.007\n",
      "iteration: 43300  accuracy:  0.8242  improvement:  0.0012\n",
      "iteration: 43400  accuracy:  0.8233  improvement:  -0.0009\n",
      "iteration: 43500  accuracy:  0.8212  improvement:  -0.0021\n",
      "iteration: 43600  accuracy:  0.8193  improvement:  -0.0019\n",
      "iteration: 43700  accuracy:  0.8136  improvement:  -0.0057\n",
      "iteration: 43800  accuracy:  0.818  improvement:  0.0044\n",
      "iteration: 43900  accuracy:  0.8235  improvement:  0.0055\n",
      "iteration: 44000  accuracy:  0.817  improvement:  -0.0065\n",
      "iteration: 44100  accuracy:  0.8184  improvement:  0.0014\n",
      "iteration: 44200  accuracy:  0.8243  improvement:  0.0059\n",
      "iteration: 44300  accuracy:  0.8113  improvement:  -0.013\n",
      "iteration: 44400  accuracy:  0.8252  improvement:  0.0139\n",
      "iteration: 44500  accuracy:  0.8157  improvement:  -0.0095\n",
      "iteration: 44600  accuracy:  0.8218  improvement:  0.0061\n",
      "iteration: 44700  accuracy:  0.8202  improvement:  -0.0016\n",
      "iteration: 44800  accuracy:  0.8158  improvement:  -0.0044\n",
      "iteration: 44900  accuracy:  0.8238  improvement:  0.008\n",
      "iteration: 45000  accuracy:  0.8252  improvement:  0.0014\n",
      "iteration: 45100  accuracy:  0.8218  improvement:  -0.0034\n",
      "iteration: 45200  accuracy:  0.824  improvement:  0.0022\n",
      "iteration: 45300  accuracy:  0.8205  improvement:  -0.0035\n",
      "# 45400  accuracy:  0.8275  improvement:  0.007 best train accuracy: 0.8275 validation accuracy:  0.7194 0.7278\n",
      "iteration: 45500  accuracy:  0.8172  improvement:  -0.0103\n",
      "iteration: 45600  accuracy:  0.8189  improvement:  0.0017\n",
      "iteration: 45700  accuracy:  0.8267  improvement:  0.0078\n",
      "iteration: 45800  accuracy:  0.8252  improvement:  -0.0015\n",
      "iteration: 45900  accuracy:  0.8274  improvement:  0.0022\n",
      "iteration: 46000  accuracy:  0.8248  improvement:  -0.0026\n",
      "# 46100  accuracy:  0.8317  improvement:  0.0069 best train accuracy: 0.8317 validation accuracy:  0.7231 0.7278\n",
      "iteration: 46200  accuracy:  0.8225  improvement:  -0.0092\n",
      "iteration: 46300  accuracy:  0.8221  improvement:  -0.0004\n",
      "iteration: 46400  accuracy:  0.8245  improvement:  0.0024\n",
      "iteration: 46500  accuracy:  0.8243  improvement:  -0.0002\n",
      "iteration: 46600  accuracy:  0.8265  improvement:  0.0022\n",
      "iteration: 46700  accuracy:  0.821  improvement:  -0.0055\n",
      "iteration: 46800  accuracy:  0.8284  improvement:  0.0074\n",
      "iteration: 46900  accuracy:  0.8081  improvement:  -0.0203\n",
      "iteration: 47000  accuracy:  0.8132  improvement:  0.0051\n",
      "iteration: 47100  accuracy:  0.8215  improvement:  0.0083\n",
      "iteration: 47200  accuracy:  0.8284  improvement:  0.0069\n",
      "iteration: 47300  accuracy:  0.8252  improvement:  -0.0032\n",
      "iteration: 47400  accuracy:  0.8297  improvement:  0.0045\n",
      "iteration: 47500  accuracy:  0.8278  improvement:  -0.0019\n",
      "iteration: 47600  accuracy:  0.8172  improvement:  -0.0106\n",
      "iteration: 47700  accuracy:  0.8275  improvement:  0.0103\n",
      "iteration: 47800  accuracy:  0.8242  improvement:  -0.0033\n",
      "iteration: 47900  accuracy:  0.8227  improvement:  -0.0015\n",
      "iteration: 48000  accuracy:  0.8249  improvement:  0.0022\n",
      "iteration: 48100  accuracy:  0.8293  improvement:  0.0044\n",
      "iteration: 48200  accuracy:  0.8273  improvement:  -0.002\n",
      "iteration: 48300  accuracy:  0.8254  improvement:  -0.0019\n",
      "iteration: 48400  accuracy:  0.8257  improvement:  0.0003\n",
      "iteration: 48500  accuracy:  0.829  improvement:  0.0033\n",
      "iteration: 48600  accuracy:  0.8269  improvement:  -0.0021\n",
      "iteration: 48700  accuracy:  0.829  improvement:  0.0021\n",
      "iteration: 48800  accuracy:  0.819  improvement:  -0.01\n",
      "iteration: 48900  accuracy:  0.828  improvement:  0.009\n",
      "iteration: 49000  accuracy:  0.8298  improvement:  0.0018\n",
      "iteration: 49100  accuracy:  0.823  improvement:  -0.0068\n",
      "iteration: 49200  accuracy:  0.8265  improvement:  0.0035\n",
      "# 49300  accuracy:  0.8331  improvement:  0.0066 best train accuracy: 0.8331 validation accuracy:  0.7263 0.7278\n",
      "iteration: 49400  accuracy:  0.8234  improvement:  -0.0097\n",
      "iteration: 49500  accuracy:  0.825  improvement:  0.0016\n",
      "iteration: 49600  accuracy:  0.8305  improvement:  0.0055\n",
      "iteration: 49700  accuracy:  0.8233  improvement:  -0.0072\n",
      "iteration: 49800  accuracy:  0.8291  improvement:  0.0058\n",
      "iteration: 49900  accuracy:  0.8236  improvement:  -0.0055\n",
      "iteration: 50000  accuracy:  0.8257  improvement:  0.0021\n",
      "iteration: 50100  accuracy:  0.8322  improvement:  0.0065\n",
      "iteration: 50200  accuracy:  0.8273  improvement:  -0.0049\n",
      "iteration: 50300  accuracy:  0.8262  improvement:  -0.0011\n",
      "iteration: 50400  accuracy:  0.8317  improvement:  0.0055\n",
      "iteration: 50500  accuracy:  0.8306  improvement:  -0.0011\n",
      "iteration: 50600  accuracy:  0.8266  improvement:  -0.004\n",
      "iteration: 50700  accuracy:  0.8169  improvement:  -0.0097\n",
      "iteration: 50800  accuracy:  0.8191  improvement:  0.0022\n",
      "iteration: 50900  accuracy:  0.8239  improvement:  0.0048\n",
      "iteration: 51000  accuracy:  0.831  improvement:  0.0071\n",
      "iteration: 51100  accuracy:  0.8227  improvement:  -0.0083\n",
      "iteration: 51200  accuracy:  0.8225  improvement:  -0.0002\n",
      "iteration: 51300  accuracy:  0.8236  improvement:  0.0011\n",
      "iteration: 51400  accuracy:  0.8194  improvement:  -0.0042\n",
      "iteration: 51500  accuracy:  0.824  improvement:  0.0046\n",
      "iteration: 51600  accuracy:  0.8245  improvement:  0.0005\n",
      "iteration: 51700  accuracy:  0.8312  improvement:  0.0067\n",
      "iteration: 51800  accuracy:  0.8288  improvement:  -0.0024\n",
      "iteration: 51900  accuracy:  0.8242  improvement:  -0.0046\n",
      "iteration: 52000  accuracy:  0.8312  improvement:  0.007\n",
      "iteration: 52100  accuracy:  0.8176  improvement:  -0.0136\n",
      "iteration: 52200  accuracy:  0.8282  improvement:  0.0106\n",
      "iteration: 52300  accuracy:  0.8267  improvement:  -0.0015\n",
      "iteration: 52400  accuracy:  0.8203  improvement:  -0.0064\n",
      "iteration: 52500  accuracy:  0.8265  improvement:  0.0062\n",
      "iteration: 52600  accuracy:  0.8277  improvement:  0.0012\n",
      "iteration: 52700  accuracy:  0.833  improvement:  0.0053\n",
      "iteration: 52800  accuracy:  0.8252  improvement:  -0.0078\n",
      "iteration: 52900  accuracy:  0.8223  improvement:  -0.0029\n",
      "iteration: 53000  accuracy:  0.8298  improvement:  0.0075\n",
      "iteration: 53100  accuracy:  0.8269  improvement:  -0.0029\n",
      "iteration: 53200  accuracy:  0.8301  improvement:  0.0032\n",
      "iteration: 53300  accuracy:  0.8205  improvement:  -0.0096\n",
      "iteration: 53400  accuracy:  0.8304  improvement:  0.0099\n",
      "iteration: 53500  accuracy:  0.8274  improvement:  -0.003\n",
      "# 53600  accuracy:  0.8333  improvement:  0.0059 best train accuracy: 0.8333 validation accuracy:  0.7242 0.7278\n",
      "iteration: 53700  accuracy:  0.8328  improvement:  -0.0005\n",
      "iteration: 53800  accuracy:  0.8271  improvement:  -0.0057\n",
      "# 53900  accuracy:  0.8371  improvement:  0.01 best train accuracy: 0.8371 validation accuracy:  0.7236 0.7278\n",
      "iteration: 54000  accuracy:  0.8344  improvement:  -0.0027\n",
      "iteration: 54100  accuracy:  0.8282  improvement:  -0.0062\n",
      "iteration: 54200  accuracy:  0.8334  improvement:  0.0052\n",
      "iteration: 54300  accuracy:  0.8296  improvement:  -0.0038\n",
      "iteration: 54400  accuracy:  0.8294  improvement:  -0.0002\n",
      "iteration: 54500  accuracy:  0.8298  improvement:  0.0004\n",
      "iteration: 54600  accuracy:  0.8169  improvement:  -0.0129\n",
      "iteration: 54700  accuracy:  0.8344  improvement:  0.0175\n",
      "iteration: 54800  accuracy:  0.824  improvement:  -0.0104\n",
      "iteration: 54900  accuracy:  0.8298  improvement:  0.0058\n",
      "iteration: 55000  accuracy:  0.836  improvement:  0.0062\n",
      "iteration: 55100  accuracy:  0.8303  improvement:  -0.0057\n",
      "iteration: 55200  accuracy:  0.8283  improvement:  -0.002\n",
      "iteration: 55300  accuracy:  0.827  improvement:  -0.0013\n",
      "iteration: 55400  accuracy:  0.8292  improvement:  0.0022\n",
      "iteration: 55500  accuracy:  0.8277  improvement:  -0.0015\n",
      "iteration: 55600  accuracy:  0.8323  improvement:  0.0046\n",
      "iteration: 55700  accuracy:  0.8358  improvement:  0.0035\n",
      "iteration: 55800  accuracy:  0.8328  improvement:  -0.003\n",
      "iteration: 55900  accuracy:  0.8326  improvement:  -0.0002\n",
      "iteration: 56000  accuracy:  0.8331  improvement:  0.0005\n",
      "iteration: 56100  accuracy:  0.8306  improvement:  -0.0025\n",
      "iteration: 56200  accuracy:  0.828  improvement:  -0.0026\n",
      "iteration: 56300  accuracy:  0.8303  improvement:  0.0023\n",
      "iteration: 56400  accuracy:  0.8219  improvement:  -0.0084\n",
      "iteration: 56500  accuracy:  0.8272  improvement:  0.0053\n",
      "iteration: 56600  accuracy:  0.8295  improvement:  0.0023\n",
      "iteration: 56700  accuracy:  0.8336  improvement:  0.0041\n",
      "iteration: 56800  accuracy:  0.8323  improvement:  -0.0013\n",
      "iteration: 56900  accuracy:  0.8322  improvement:  -1e-04\n",
      "iteration: 57000  accuracy:  0.8294  improvement:  -0.0028\n",
      "iteration: 57100  accuracy:  0.8293  improvement:  -1e-04\n",
      "iteration: 57200  accuracy:  0.8303  improvement:  0.001\n",
      "iteration: 57300  accuracy:  0.835  improvement:  0.0047\n",
      "iteration: 57400  accuracy:  0.8286  improvement:  -0.0064\n",
      "iteration: 57500  accuracy:  0.8246  improvement:  -0.004\n",
      "iteration: 57600  accuracy:  0.8292  improvement:  0.0046\n",
      "iteration: 57700  accuracy:  0.8363  improvement:  0.0071\n",
      "iteration: 57800  accuracy:  0.8233  improvement:  -0.013\n",
      "iteration: 57900  accuracy:  0.8288  improvement:  0.0055\n",
      "iteration: 58000  accuracy:  0.8268  improvement:  -0.002\n",
      "iteration: 58100  accuracy:  0.831  improvement:  0.0042\n",
      "iteration: 58200  accuracy:  0.8342  improvement:  0.0032\n",
      "iteration: 58300  accuracy:  0.832  improvement:  -0.0022\n",
      "iteration: 58400  accuracy:  0.8322  improvement:  0.0002\n",
      "iteration: 58500  accuracy:  0.8354  improvement:  0.0032\n",
      "iteration: 58600  accuracy:  0.8288  improvement:  -0.0066\n",
      "iteration: 58700  accuracy:  0.8323  improvement:  0.0035\n",
      "iteration: 58800  accuracy:  0.8336  improvement:  0.0013\n",
      "iteration: 58900  accuracy:  0.8359  improvement:  0.0023\n",
      "iteration: 59000  accuracy:  0.8337  improvement:  -0.0022\n",
      "iteration: 59100  accuracy:  0.8296  improvement:  -0.0041\n",
      "iteration: 59200  accuracy:  0.8301  improvement:  0.0005\n",
      "iteration: 59300  accuracy:  0.8363  improvement:  0.0062\n",
      "iteration: 59400  accuracy:  0.8326  improvement:  -0.0037\n",
      "iteration: 59500  accuracy:  0.8312  improvement:  -0.0014\n",
      "iteration: 59600  accuracy:  0.8328  improvement:  0.0016\n",
      "iteration: 59700  accuracy:  0.8314  improvement:  -0.0014\n",
      "iteration: 59800  accuracy:  0.8304  improvement:  -0.001\n",
      "iteration: 59900  accuracy:  0.8302  improvement:  -0.0002\n",
      "iteration: 60000  accuracy:  0.8318  improvement:  0.0016\n",
      "iteration: 60100  accuracy:  0.8339  improvement:  0.0021\n",
      "iteration: 60200  accuracy:  0.8326  improvement:  -0.0013\n",
      "iteration: 60300  accuracy:  0.8349  improvement:  0.0023\n",
      "iteration: 60400  accuracy:  0.8314  improvement:  -0.0035\n",
      "iteration: 60500  accuracy:  0.8275  improvement:  -0.0039\n",
      "iteration: 60600  accuracy:  0.8354  improvement:  0.0079\n",
      "iteration: 60700  accuracy:  0.832  improvement:  -0.0034\n",
      "iteration: 60800  accuracy:  0.8354  improvement:  0.0034\n",
      "iteration: 60900  accuracy:  0.8265  improvement:  -0.0089\n",
      "iteration: 61000  accuracy:  0.8277  improvement:  0.0012\n",
      "iteration: 61100  accuracy:  0.8321  improvement:  0.0044\n",
      "# 61200  accuracy:  0.8381  improvement:  0.006 best train accuracy: 0.8381 validation accuracy:  0.7261 0.7278\n",
      "iteration: 61300  accuracy:  0.833  improvement:  -0.0051\n",
      "iteration: 61400  accuracy:  0.8367  improvement:  0.0037\n",
      "iteration: 61500  accuracy:  0.8344  improvement:  -0.0023\n",
      "iteration: 61600  accuracy:  0.8288  improvement:  -0.0056\n",
      "iteration: 61700  accuracy:  0.834  improvement:  0.0052\n",
      "iteration: 61800  accuracy:  0.8329  improvement:  -0.0011\n",
      "iteration: 61900  accuracy:  0.8378  improvement:  0.0049\n",
      "iteration: 62000  accuracy:  0.8378  improvement:  0.0\n",
      "iteration: 62100  accuracy:  0.8352  improvement:  -0.0026\n",
      "iteration: 62200  accuracy:  0.8341  improvement:  -0.0011\n",
      "iteration: 62300  accuracy:  0.8326  improvement:  -0.0015\n",
      "iteration: 62400  accuracy:  0.8346  improvement:  0.002\n",
      "# 62500  accuracy:  0.8382  improvement:  0.0036 best train accuracy: 0.8382 validation accuracy:  0.7229 0.7278\n",
      "iteration: 62600  accuracy:  0.8348  improvement:  -0.0034\n",
      "iteration: 62700  accuracy:  0.8286  improvement:  -0.0062\n",
      "iteration: 62800  accuracy:  0.8365  improvement:  0.0079\n",
      "iteration: 62900  accuracy:  0.8365  improvement:  0.0\n",
      "iteration: 63000  accuracy:  0.8374  improvement:  0.0009\n",
      "# 63100  accuracy:  0.8385  improvement:  0.0011 best train accuracy: 0.8385 validation accuracy:  0.7263 0.7278\n",
      "iteration: 63200  accuracy:  0.8315  improvement:  -0.007\n",
      "iteration: 63300  accuracy:  0.835  improvement:  0.0035\n",
      "iteration: 63400  accuracy:  0.8321  improvement:  -0.0029\n",
      "# 63500  accuracy:  0.8395  improvement:  0.0074 best train accuracy: 0.8395 validation accuracy:  0.7322 0.7322\n",
      "iteration: 63600  accuracy:  0.8374  improvement:  -0.0021\n",
      "iteration: 63700  accuracy:  0.8352  improvement:  -0.0022\n",
      "# 63800  accuracy:  0.8403  improvement:  0.0051 best train accuracy: 0.8403 validation accuracy:  0.7273 0.7322\n",
      "iteration: 63900  accuracy:  0.8271  improvement:  -0.0132\n",
      "iteration: 64000  accuracy:  0.8378  improvement:  0.0107\n",
      "iteration: 64100  accuracy:  0.8351  improvement:  -0.0027\n",
      "iteration: 64200  accuracy:  0.837  improvement:  0.0019\n",
      "iteration: 64300  accuracy:  0.8367  improvement:  -0.0003\n",
      "iteration: 64400  accuracy:  0.8294  improvement:  -0.0073\n",
      "iteration: 64500  accuracy:  0.8348  improvement:  0.0054\n",
      "iteration: 64600  accuracy:  0.8334  improvement:  -0.0014\n",
      "iteration: 64700  accuracy:  0.8382  improvement:  0.0048\n",
      "iteration: 64800  accuracy:  0.8372  improvement:  -0.001\n",
      "iteration: 64900  accuracy:  0.8338  improvement:  -0.0034\n",
      "iteration: 65000  accuracy:  0.8342  improvement:  0.0004\n",
      "iteration: 65100  accuracy:  0.8377  improvement:  0.0035\n",
      "iteration: 65200  accuracy:  0.8326  improvement:  -0.0051\n",
      "iteration: 65300  accuracy:  0.8358  improvement:  0.0032\n",
      "iteration: 65400  accuracy:  0.8376  improvement:  0.0018\n",
      "iteration: 65500  accuracy:  0.8374  improvement:  -0.0002\n",
      "iteration: 65600  accuracy:  0.8308  improvement:  -0.0066\n",
      "iteration: 65700  accuracy:  0.835  improvement:  0.0042\n",
      "iteration: 65800  accuracy:  0.8353  improvement:  0.0003\n",
      "iteration: 65900  accuracy:  0.8359  improvement:  0.0006\n",
      "# 66000  accuracy:  0.841  improvement:  0.0051 best train accuracy: 0.841 validation accuracy:  0.7267 0.7322\n",
      "iteration: 66100  accuracy:  0.836  improvement:  -0.005\n",
      "iteration: 66200  accuracy:  0.8333  improvement:  -0.0027\n",
      "iteration: 66300  accuracy:  0.8294  improvement:  -0.0039\n",
      "iteration: 66400  accuracy:  0.8344  improvement:  0.005\n",
      "# 66500  accuracy:  0.8425  improvement:  0.0081 best train accuracy: 0.8425 validation accuracy:  0.7277 0.7322\n",
      "iteration: 66600  accuracy:  0.8363  improvement:  -0.0062\n",
      "iteration: 66700  accuracy:  0.8356  improvement:  -0.0007\n",
      "iteration: 66800  accuracy:  0.8382  improvement:  0.0026\n",
      "iteration: 66900  accuracy:  0.8328  improvement:  -0.0054\n",
      "iteration: 67000  accuracy:  0.8371  improvement:  0.0043\n",
      "iteration: 67100  accuracy:  0.8321  improvement:  -0.005\n",
      "iteration: 67200  accuracy:  0.8358  improvement:  0.0037\n",
      "iteration: 67300  accuracy:  0.8376  improvement:  0.0018\n",
      "iteration: 67400  accuracy:  0.8364  improvement:  -0.0012\n",
      "iteration: 67500  accuracy:  0.8359  improvement:  -0.0005\n",
      "iteration: 67600  accuracy:  0.8373  improvement:  0.0014\n",
      "iteration: 67700  accuracy:  0.8363  improvement:  -0.001\n",
      "iteration: 67800  accuracy:  0.8355  improvement:  -0.0008\n",
      "iteration: 67900  accuracy:  0.8267  improvement:  -0.0088\n",
      "iteration: 68000  accuracy:  0.8343  improvement:  0.0076\n",
      "iteration: 68100  accuracy:  0.8335  improvement:  -0.0008\n",
      "iteration: 68200  accuracy:  0.8358  improvement:  0.0023\n",
      "iteration: 68300  accuracy:  0.8316  improvement:  -0.0042\n",
      "iteration: 68400  accuracy:  0.8386  improvement:  0.007\n",
      "iteration: 68500  accuracy:  0.839  improvement:  0.0004\n",
      "iteration: 68600  accuracy:  0.8384  improvement:  -0.0006\n",
      "iteration: 68700  accuracy:  0.8404  improvement:  0.002\n",
      "iteration: 68800  accuracy:  0.8358  improvement:  -0.0046\n",
      "iteration: 68900  accuracy:  0.8412  improvement:  0.0054\n",
      "iteration: 69000  accuracy:  0.8367  improvement:  -0.0045\n",
      "iteration: 69100  accuracy:  0.8345  improvement:  -0.0022\n",
      "iteration: 69200  accuracy:  0.8372  improvement:  0.0027\n",
      "iteration: 69300  accuracy:  0.8382  improvement:  0.001\n",
      "iteration: 69400  accuracy:  0.842  improvement:  0.0038\n",
      "iteration: 69500  accuracy:  0.8402  improvement:  -0.0018\n",
      "iteration: 69600  accuracy:  0.8404  improvement:  0.0002\n",
      "iteration: 69700  accuracy:  0.8343  improvement:  -0.0061\n",
      "iteration: 69800  accuracy:  0.8364  improvement:  0.0021\n",
      "# 69900  accuracy:  0.8428  improvement:  0.0064 best train accuracy: 0.8428 validation accuracy:  0.7276 0.7322\n",
      "iteration: 70000  accuracy:  0.8396  improvement:  -0.0032\n",
      "iteration: 70100  accuracy:  0.8391  improvement:  -0.0005\n",
      "iteration: 70200  accuracy:  0.8315  improvement:  -0.0076\n",
      "iteration: 70300  accuracy:  0.8299  improvement:  -0.0016\n",
      "iteration: 70400  accuracy:  0.8336  improvement:  0.0037\n",
      "iteration: 70500  accuracy:  0.8378  improvement:  0.0042\n",
      "iteration: 70600  accuracy:  0.8385  improvement:  0.0007\n",
      "iteration: 70700  accuracy:  0.8391  improvement:  0.0006\n",
      "iteration: 70800  accuracy:  0.8413  improvement:  0.0022\n",
      "iteration: 70900  accuracy:  0.8333  improvement:  -0.008\n",
      "iteration: 71000  accuracy:  0.8394  improvement:  0.0061\n",
      "iteration: 71100  accuracy:  0.836  improvement:  -0.0034\n",
      "iteration: 71200  accuracy:  0.8376  improvement:  0.0016\n",
      "iteration: 71300  accuracy:  0.8377  improvement:  1e-04\n",
      "iteration: 71400  accuracy:  0.8356  improvement:  -0.0021\n",
      "iteration: 71500  accuracy:  0.839  improvement:  0.0034\n",
      "iteration: 71600  accuracy:  0.8407  improvement:  0.0017\n",
      "iteration: 71700  accuracy:  0.8397  improvement:  -0.001\n",
      "iteration: 71800  accuracy:  0.8412  improvement:  0.0015\n",
      "iteration: 71900  accuracy:  0.8398  improvement:  -0.0014\n",
      "iteration: 72000  accuracy:  0.836  improvement:  -0.0038\n",
      "iteration: 72100  accuracy:  0.8377  improvement:  0.0017\n",
      "iteration: 72200  accuracy:  0.8359  improvement:  -0.0018\n",
      "iteration: 72300  accuracy:  0.8419  improvement:  0.006\n",
      "iteration: 72400  accuracy:  0.841  improvement:  -0.0009\n",
      "iteration: 72500  accuracy:  0.8328  improvement:  -0.0082\n",
      "iteration: 72600  accuracy:  0.8396  improvement:  0.0068\n",
      "iteration: 72700  accuracy:  0.8341  improvement:  -0.0055\n",
      "iteration: 72800  accuracy:  0.8381  improvement:  0.004\n",
      "iteration: 72900  accuracy:  0.821  improvement:  -0.0171\n",
      "iteration: 73000  accuracy:  0.8397  improvement:  0.0187\n",
      "iteration: 73100  accuracy:  0.8358  improvement:  -0.0039\n",
      "iteration: 73200  accuracy:  0.8379  improvement:  0.0021\n",
      "iteration: 73300  accuracy:  0.8422  improvement:  0.0043\n",
      "iteration: 73400  accuracy:  0.8382  improvement:  -0.004\n",
      "iteration: 73500  accuracy:  0.8415  improvement:  0.0033\n",
      "# 73600  accuracy:  0.8459  improvement:  0.0044 best train accuracy: 0.8459 validation accuracy:  0.7315 0.7322\n",
      "iteration: 73700  accuracy:  0.84  improvement:  -0.0059\n",
      "iteration: 73800  accuracy:  0.837  improvement:  -0.003\n",
      "iteration: 73900  accuracy:  0.8418  improvement:  0.0048\n",
      "iteration: 74000  accuracy:  0.843  improvement:  0.0012\n",
      "iteration: 74100  accuracy:  0.8401  improvement:  -0.0029\n",
      "iteration: 74200  accuracy:  0.8434  improvement:  0.0033\n",
      "iteration: 74300  accuracy:  0.8391  improvement:  -0.0043\n",
      "iteration: 74400  accuracy:  0.84  improvement:  0.0009\n",
      "iteration: 74500  accuracy:  0.8427  improvement:  0.0027\n",
      "iteration: 74600  accuracy:  0.8323  improvement:  -0.0104\n",
      "iteration: 74700  accuracy:  0.8409  improvement:  0.0086\n",
      "iteration: 74800  accuracy:  0.8414  improvement:  0.0005\n",
      "iteration: 74900  accuracy:  0.8427  improvement:  0.0013\n",
      "iteration: 75000  accuracy:  0.8429  improvement:  0.0002\n",
      "iteration: 75100  accuracy:  0.845  improvement:  0.0021\n",
      "iteration: 75200  accuracy:  0.8403  improvement:  -0.0047\n",
      "iteration: 75300  accuracy:  0.8401  improvement:  -0.0002\n",
      "iteration: 75400  accuracy:  0.8302  improvement:  -0.0099\n",
      "iteration: 75500  accuracy:  0.836  improvement:  0.0058\n",
      "iteration: 75600  accuracy:  0.8414  improvement:  0.0054\n",
      "iteration: 75700  accuracy:  0.8372  improvement:  -0.0042\n",
      "iteration: 75800  accuracy:  0.8386  improvement:  0.0014\n",
      "iteration: 75900  accuracy:  0.842  improvement:  0.0034\n",
      "iteration: 76000  accuracy:  0.8381  improvement:  -0.0039\n",
      "iteration: 76100  accuracy:  0.8418  improvement:  0.0037\n",
      "iteration: 76200  accuracy:  0.841  improvement:  -0.0008\n",
      "iteration: 76300  accuracy:  0.8388  improvement:  -0.0022\n",
      "iteration: 76400  accuracy:  0.8366  improvement:  -0.0022\n",
      "iteration: 76500  accuracy:  0.8372  improvement:  0.0006\n",
      "iteration: 76600  accuracy:  0.8379  improvement:  0.0007\n",
      "iteration: 76700  accuracy:  0.8341  improvement:  -0.0038\n",
      "iteration: 76800  accuracy:  0.8376  improvement:  0.0035\n",
      "iteration: 76900  accuracy:  0.8402  improvement:  0.0026\n",
      "iteration: 77000  accuracy:  0.8419  improvement:  0.0017\n",
      "iteration: 77100  accuracy:  0.8427  improvement:  0.0008\n",
      "iteration: 77200  accuracy:  0.8409  improvement:  -0.0018\n",
      "iteration: 77300  accuracy:  0.8413  improvement:  0.0004\n",
      "iteration: 77400  accuracy:  0.8359  improvement:  -0.0054\n",
      "iteration: 77500  accuracy:  0.8402  improvement:  0.0043\n",
      "iteration: 77600  accuracy:  0.8371  improvement:  -0.0031\n",
      "iteration: 77700  accuracy:  0.8404  improvement:  0.0033\n",
      "# 77800  accuracy:  0.8464  improvement:  0.006 best train accuracy: 0.8464 validation accuracy:  0.728 0.7322\n",
      "iteration: 77900  accuracy:  0.8371  improvement:  -0.0093\n",
      "iteration: 78000  accuracy:  0.8412  improvement:  0.0041\n",
      "iteration: 78100  accuracy:  0.8458  improvement:  0.0046\n",
      "iteration: 78200  accuracy:  0.8368  improvement:  -0.009\n",
      "iteration: 78300  accuracy:  0.8449  improvement:  0.0081\n",
      "iteration: 78400  accuracy:  0.8422  improvement:  -0.0027\n",
      "iteration: 78500  accuracy:  0.8424  improvement:  0.0002\n",
      "iteration: 78600  accuracy:  0.8329  improvement:  -0.0095\n",
      "iteration: 78700  accuracy:  0.8336  improvement:  0.0007\n",
      "iteration: 78800  accuracy:  0.8435  improvement:  0.0099\n",
      "iteration: 78900  accuracy:  0.8411  improvement:  -0.0024\n",
      "iteration: 79000  accuracy:  0.8352  improvement:  -0.0059\n",
      "iteration: 79100  accuracy:  0.8419  improvement:  0.0067\n",
      "iteration: 79200  accuracy:  0.8446  improvement:  0.0027\n",
      "iteration: 79300  accuracy:  0.8418  improvement:  -0.0028\n",
      "iteration: 79400  accuracy:  0.8454  improvement:  0.0036\n",
      "iteration: 79500  accuracy:  0.8453  improvement:  -1e-04\n",
      "iteration: 79600  accuracy:  0.8286  improvement:  -0.0167\n",
      "iteration: 79700  accuracy:  0.8421  improvement:  0.0135\n",
      "iteration: 79800  accuracy:  0.8394  improvement:  -0.0027\n",
      "iteration: 79900  accuracy:  0.8388  improvement:  -0.0006\n",
      "iteration: 80000  accuracy:  0.836  improvement:  -0.0028\n",
      "iteration: 80100  accuracy:  0.8429  improvement:  0.0069\n",
      "iteration: 80200  accuracy:  0.8356  improvement:  -0.0073\n",
      "iteration: 80300  accuracy:  0.8391  improvement:  0.0035\n",
      "iteration: 80400  accuracy:  0.8347  improvement:  -0.0044\n",
      "iteration: 80500  accuracy:  0.8402  improvement:  0.0055\n",
      "iteration: 80600  accuracy:  0.8384  improvement:  -0.0018\n",
      "iteration: 80700  accuracy:  0.8421  improvement:  0.0037\n",
      "iteration: 80800  accuracy:  0.8352  improvement:  -0.0069\n",
      "# 80900  accuracy:  0.8487  improvement:  0.0135 best train accuracy: 0.8487 validation accuracy:  0.728 0.7322\n",
      "iteration: 81000  accuracy:  0.8427  improvement:  -0.006\n",
      "iteration: 81100  accuracy:  0.8411  improvement:  -0.0016\n",
      "iteration: 81200  accuracy:  0.8444  improvement:  0.0033\n",
      "iteration: 81300  accuracy:  0.8418  improvement:  -0.0026\n",
      "iteration: 81400  accuracy:  0.845  improvement:  0.0032\n",
      "iteration: 81500  accuracy:  0.8424  improvement:  -0.0026\n",
      "iteration: 81600  accuracy:  0.8441  improvement:  0.0017\n",
      "iteration: 81700  accuracy:  0.8415  improvement:  -0.0026\n",
      "iteration: 81800  accuracy:  0.8338  improvement:  -0.0077\n",
      "iteration: 81900  accuracy:  0.8388  improvement:  0.005\n",
      "iteration: 82000  accuracy:  0.836  improvement:  -0.0028\n",
      "iteration: 82100  accuracy:  0.8455  improvement:  0.0095\n",
      "iteration: 82200  accuracy:  0.8331  improvement:  -0.0124\n",
      "iteration: 82300  accuracy:  0.8439  improvement:  0.0108\n",
      "iteration: 82400  accuracy:  0.839  improvement:  -0.0049\n",
      "iteration: 82500  accuracy:  0.8466  improvement:  0.0076\n",
      "iteration: 82600  accuracy:  0.8435  improvement:  -0.0031\n",
      "iteration: 82700  accuracy:  0.8381  improvement:  -0.0054\n",
      "iteration: 82800  accuracy:  0.8443  improvement:  0.0062\n",
      "iteration: 82900  accuracy:  0.8417  improvement:  -0.0026\n",
      "iteration: 83000  accuracy:  0.8382  improvement:  -0.0035\n",
      "iteration: 83100  accuracy:  0.8377  improvement:  -0.0005\n",
      "iteration: 83200  accuracy:  0.8425  improvement:  0.0048\n",
      "iteration: 83300  accuracy:  0.8346  improvement:  -0.0079\n",
      "iteration: 83400  accuracy:  0.84  improvement:  0.0054\n",
      "iteration: 83500  accuracy:  0.838  improvement:  -0.002\n",
      "iteration: 83600  accuracy:  0.8415  improvement:  0.0035\n",
      "iteration: 83700  accuracy:  0.8439  improvement:  0.0024\n",
      "iteration: 83800  accuracy:  0.8348  improvement:  -0.0091\n",
      "iteration: 83900  accuracy:  0.8448  improvement:  0.01\n",
      "iteration: 84000  accuracy:  0.8381  improvement:  -0.0067\n",
      "iteration: 84100  accuracy:  0.8358  improvement:  -0.0023\n",
      "iteration: 84200  accuracy:  0.8447  improvement:  0.0089\n",
      "# 84300  accuracy:  0.8512  improvement:  0.0065 best train accuracy: 0.8512 validation accuracy:  0.7266 0.7322\n",
      "iteration: 84400  accuracy:  0.8333  improvement:  -0.0179\n",
      "iteration: 84500  accuracy:  0.8448  improvement:  0.0115\n",
      "iteration: 84600  accuracy:  0.8436  improvement:  -0.0012\n",
      "iteration: 84700  accuracy:  0.8456  improvement:  0.002\n",
      "iteration: 84800  accuracy:  0.845  improvement:  -0.0006\n",
      "iteration: 84900  accuracy:  0.8415  improvement:  -0.0035\n",
      "iteration: 85000  accuracy:  0.8451  improvement:  0.0036\n",
      "iteration: 85100  accuracy:  0.8376  improvement:  -0.0075\n",
      "iteration: 85200  accuracy:  0.847  improvement:  0.0094\n",
      "iteration: 85300  accuracy:  0.8469  improvement:  -1e-04\n",
      "iteration: 85400  accuracy:  0.845  improvement:  -0.0019\n",
      "iteration: 85500  accuracy:  0.8394  improvement:  -0.0056\n",
      "iteration: 85600  accuracy:  0.8487  improvement:  0.0093\n",
      "iteration: 85700  accuracy:  0.8431  improvement:  -0.0056\n",
      "iteration: 85800  accuracy:  0.8408  improvement:  -0.0023\n",
      "iteration: 85900  accuracy:  0.834  improvement:  -0.0068\n",
      "iteration: 86000  accuracy:  0.8424  improvement:  0.0084\n",
      "iteration: 86100  accuracy:  0.8448  improvement:  0.0024\n",
      "iteration: 86200  accuracy:  0.8465  improvement:  0.0017\n",
      "iteration: 86300  accuracy:  0.8481  improvement:  0.0016\n",
      "iteration: 86400  accuracy:  0.8477  improvement:  -0.0004\n",
      "iteration: 86500  accuracy:  0.8386  improvement:  -0.0091\n",
      "iteration: 86600  accuracy:  0.8441  improvement:  0.0055\n",
      "iteration: 86700  accuracy:  0.8452  improvement:  0.0011\n",
      "iteration: 86800  accuracy:  0.8467  improvement:  0.0015\n",
      "iteration: 86900  accuracy:  0.8466  improvement:  -1e-04\n",
      "iteration: 87000  accuracy:  0.844  improvement:  -0.0026\n",
      "iteration: 87100  accuracy:  0.8475  improvement:  0.0035\n",
      "iteration: 87200  accuracy:  0.8425  improvement:  -0.005\n",
      "iteration: 87300  accuracy:  0.8468  improvement:  0.0043\n",
      "iteration: 87400  accuracy:  0.8452  improvement:  -0.0016\n",
      "iteration: 87500  accuracy:  0.8417  improvement:  -0.0035\n",
      "iteration: 87600  accuracy:  0.8497  improvement:  0.008\n",
      "iteration: 87700  accuracy:  0.8451  improvement:  -0.0046\n",
      "iteration: 87800  accuracy:  0.8418  improvement:  -0.0033\n",
      "iteration: 87900  accuracy:  0.8486  improvement:  0.0068\n",
      "iteration: 88000  accuracy:  0.8375  improvement:  -0.0111\n",
      "iteration: 88100  accuracy:  0.8465  improvement:  0.009\n",
      "iteration: 88200  accuracy:  0.8493  improvement:  0.0028\n",
      "iteration: 88300  accuracy:  0.8463  improvement:  -0.003\n",
      "iteration: 88400  accuracy:  0.8424  improvement:  -0.0039\n",
      "iteration: 88500  accuracy:  0.8433  improvement:  0.0009\n",
      "iteration: 88600  accuracy:  0.851  improvement:  0.0077\n",
      "iteration: 88700  accuracy:  0.8464  improvement:  -0.0046\n",
      "iteration: 88800  accuracy:  0.8375  improvement:  -0.0089\n",
      "iteration: 88900  accuracy:  0.8405  improvement:  0.003\n",
      "iteration: 89000  accuracy:  0.8445  improvement:  0.004\n",
      "iteration: 89100  accuracy:  0.8463  improvement:  0.0018\n",
      "iteration: 89200  accuracy:  0.8438  improvement:  -0.0025\n",
      "iteration: 89300  accuracy:  0.8412  improvement:  -0.0026\n",
      "iteration: 89400  accuracy:  0.8432  improvement:  0.002\n",
      "iteration: 89500  accuracy:  0.8426  improvement:  -0.0006\n",
      "iteration: 89600  accuracy:  0.8445  improvement:  0.0019\n",
      "iteration: 89700  accuracy:  0.8402  improvement:  -0.0043\n",
      "iteration: 89800  accuracy:  0.8437  improvement:  0.0035\n",
      "iteration: 89900  accuracy:  0.8385  improvement:  -0.0052\n",
      "iteration: 90000  accuracy:  0.8476  improvement:  0.0091\n",
      "iteration: 90100  accuracy:  0.8449  improvement:  -0.0027\n",
      "iteration: 90200  accuracy:  0.8382  improvement:  -0.0067\n",
      "iteration: 90300  accuracy:  0.8416  improvement:  0.0034\n",
      "iteration: 90400  accuracy:  0.8454  improvement:  0.0038\n",
      "iteration: 90500  accuracy:  0.8508  improvement:  0.0054\n",
      "iteration: 90600  accuracy:  0.8496  improvement:  -0.0012\n",
      "iteration: 90700  accuracy:  0.8464  improvement:  -0.0032\n",
      "iteration: 90800  accuracy:  0.843  improvement:  -0.0034\n",
      "iteration: 90900  accuracy:  0.8418  improvement:  -0.0012\n",
      "iteration: 91000  accuracy:  0.849  improvement:  0.0072\n",
      "iteration: 91100  accuracy:  0.8426  improvement:  -0.0064\n",
      "iteration: 91200  accuracy:  0.8467  improvement:  0.0041\n",
      "iteration: 91300  accuracy:  0.8411  improvement:  -0.0056\n",
      "iteration: 91400  accuracy:  0.8478  improvement:  0.0067\n",
      "iteration: 91500  accuracy:  0.8498  improvement:  0.002\n",
      "iteration: 91600  accuracy:  0.8503  improvement:  0.0005\n",
      "iteration: 91700  accuracy:  0.8469  improvement:  -0.0034\n",
      "iteration: 91800  accuracy:  0.8405  improvement:  -0.0064\n",
      "iteration: 91900  accuracy:  0.8476  improvement:  0.0071\n",
      "iteration: 92000  accuracy:  0.8448  improvement:  -0.0028\n",
      "iteration: 92100  accuracy:  0.8449  improvement:  1e-04\n",
      "iteration: 92200  accuracy:  0.8489  improvement:  0.004\n",
      "iteration: 92300  accuracy:  0.8343  improvement:  -0.0146\n",
      "# 92400  accuracy:  0.856  improvement:  0.0217 best train accuracy: 0.856 validation accuracy:  0.7324 0.7324\n",
      "iteration: 92500  accuracy:  0.8449  improvement:  -0.0111\n",
      "iteration: 92600  accuracy:  0.8417  improvement:  -0.0032\n",
      "iteration: 92700  accuracy:  0.8456  improvement:  0.0039\n",
      "iteration: 92800  accuracy:  0.8477  improvement:  0.0021\n",
      "iteration: 92900  accuracy:  0.8447  improvement:  -0.003\n",
      "iteration: 93000  accuracy:  0.852  improvement:  0.0073\n",
      "iteration: 93100  accuracy:  0.84  improvement:  -0.012\n",
      "iteration: 93200  accuracy:  0.8396  improvement:  -0.0004\n",
      "iteration: 93300  accuracy:  0.8494  improvement:  0.0098\n",
      "iteration: 93400  accuracy:  0.8442  improvement:  -0.0052\n",
      "iteration: 93500  accuracy:  0.8469  improvement:  0.0027\n",
      "iteration: 93600  accuracy:  0.8417  improvement:  -0.0052\n",
      "iteration: 93700  accuracy:  0.8474  improvement:  0.0057\n",
      "iteration: 93800  accuracy:  0.8423  improvement:  -0.0051\n",
      "iteration: 93900  accuracy:  0.8434  improvement:  0.0011\n",
      "iteration: 94000  accuracy:  0.8475  improvement:  0.0041\n",
      "iteration: 94100  accuracy:  0.8488  improvement:  0.0013\n",
      "iteration: 94200  accuracy:  0.8456  improvement:  -0.0032\n",
      "iteration: 94300  accuracy:  0.8444  improvement:  -0.0012\n",
      "iteration: 94400  accuracy:  0.8492  improvement:  0.0048\n",
      "iteration: 94500  accuracy:  0.8469  improvement:  -0.0023\n",
      "iteration: 94600  accuracy:  0.8463  improvement:  -0.0006\n",
      "iteration: 94700  accuracy:  0.8528  improvement:  0.0065\n",
      "iteration: 94800  accuracy:  0.8416  improvement:  -0.0112\n",
      "iteration: 94900  accuracy:  0.8365  improvement:  -0.0051\n",
      "iteration: 95000  accuracy:  0.8444  improvement:  0.0079\n",
      "iteration: 95100  accuracy:  0.8498  improvement:  0.0054\n",
      "iteration: 95200  accuracy:  0.8418  improvement:  -0.008\n",
      "iteration: 95300  accuracy:  0.8456  improvement:  0.0038\n",
      "iteration: 95400  accuracy:  0.844  improvement:  -0.0016\n",
      "iteration: 95500  accuracy:  0.8458  improvement:  0.0018\n",
      "iteration: 95600  accuracy:  0.8398  improvement:  -0.006\n",
      "iteration: 95700  accuracy:  0.8447  improvement:  0.0049\n",
      "iteration: 95800  accuracy:  0.8498  improvement:  0.0051\n",
      "iteration: 95900  accuracy:  0.8478  improvement:  -0.002\n",
      "iteration: 96000  accuracy:  0.8532  improvement:  0.0054\n",
      "iteration: 96100  accuracy:  0.8492  improvement:  -0.004\n",
      "iteration: 96200  accuracy:  0.8467  improvement:  -0.0025\n",
      "iteration: 96300  accuracy:  0.8479  improvement:  0.0012\n",
      "iteration: 96400  accuracy:  0.8498  improvement:  0.0019\n",
      "iteration: 96500  accuracy:  0.8512  improvement:  0.0014\n",
      "iteration: 96600  accuracy:  0.8386  improvement:  -0.0126\n",
      "iteration: 96700  accuracy:  0.8411  improvement:  0.0025\n",
      "iteration: 96800  accuracy:  0.8496  improvement:  0.0085\n",
      "iteration: 96900  accuracy:  0.8489  improvement:  -0.0007\n",
      "iteration: 97000  accuracy:  0.8469  improvement:  -0.002\n",
      "iteration: 97100  accuracy:  0.8468  improvement:  -1e-04\n",
      "iteration: 97200  accuracy:  0.841  improvement:  -0.0058\n",
      "iteration: 97300  accuracy:  0.8479  improvement:  0.0069\n",
      "iteration: 97400  accuracy:  0.8487  improvement:  0.0008\n",
      "iteration: 97500  accuracy:  0.8457  improvement:  -0.003\n",
      "iteration: 97600  accuracy:  0.843  improvement:  -0.0027\n",
      "iteration: 97700  accuracy:  0.8472  improvement:  0.0042\n",
      "iteration: 97800  accuracy:  0.8378  improvement:  -0.0094\n",
      "iteration: 97900  accuracy:  0.8456  improvement:  0.0078\n",
      "iteration: 98000  accuracy:  0.8484  improvement:  0.0028\n",
      "iteration: 98100  accuracy:  0.8429  improvement:  -0.0055\n",
      "iteration: 98200  accuracy:  0.8462  improvement:  0.0033\n",
      "iteration: 98300  accuracy:  0.8442  improvement:  -0.002\n",
      "iteration: 98400  accuracy:  0.85  improvement:  0.0058\n",
      "iteration: 98500  accuracy:  0.8472  improvement:  -0.0028\n",
      "iteration: 98600  accuracy:  0.8436  improvement:  -0.0036\n",
      "iteration: 98700  accuracy:  0.8454  improvement:  0.0018\n",
      "iteration: 98800  accuracy:  0.8462  improvement:  0.0008\n",
      "iteration: 98900  accuracy:  0.8436  improvement:  -0.0026\n",
      "iteration: 99000  accuracy:  0.8529  improvement:  0.0093\n",
      "iteration: 99100  accuracy:  0.8457  improvement:  -0.0072\n",
      "iteration: 99200  accuracy:  0.8508  improvement:  0.0051\n",
      "iteration: 99300  accuracy:  0.8496  improvement:  -0.0012\n",
      "iteration: 99400  accuracy:  0.8477  improvement:  -0.0019\n",
      "iteration: 99500  accuracy:  0.8488  improvement:  0.0011\n",
      "iteration: 99600  accuracy:  0.8536  improvement:  0.0048\n",
      "iteration: 99700  accuracy:  0.8416  improvement:  -0.012\n",
      "iteration: 99800  accuracy:  0.8482  improvement:  0.0066\n",
      "iteration: 99900  accuracy:  0.8488  improvement:  0.0006\n",
      "#Time usage: 0:26:53\n",
      "#Training done!\n",
      "#(Loss, Accuracy) on Training Dataset (0.5960, 0.84)\n",
      "#(Loss, Accuracy) on Validataion Dataset (1.0173, 0.73)\n"
     ]
    }
   ],
   "source": [
    "# Clear old variables\n",
    "tf.reset_default_graph()    \n",
    "\n",
    "# Declare out simple model\n",
    "model = my_model_simple1()    \n",
    "    \n",
    "# Now, create a tf.Session and train the model\n",
    "with tf.Session(config=conf) as sess:\n",
    "    ##############################################################################\n",
    "    #                          IMPLEMENT YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    run_model_v2(sess, model, X_train, Y_train, batch_size = 77, is_training=True, epochs=100000)\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    print(\"#(Loss, Accuracy) on Training Dataset (%.4f, %.2f)\" % run_model_v2(sess, model, X_train, Y_train, is_training = False))\n",
    "    print(\"#(Loss, Accuracy) on Validataion Dataset (%.4f, %.2f)\" % run_model_v2(sess, model, X_val, Y_val, is_training = False))\n",
    "    \n",
    "    #Save your final model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"./model_checkpoints/my_model_final\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfC9gvjtvN-Y"
   },
   "source": [
    "### Describe what you did here\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network.\n",
    "\n",
    "In our model, first we put one convolution layer for stem layer and put batch normalization layer to deal with gradient decreasing faster or gradient exploding. After that, we put two inception module for learning and dropout layers for generalization. Finally, we connected this inception modules serially with flatten layer and dense fully connected layer to make 10 output. And we take softmax function to 10 outputs and select what is the best classification decision. \n",
    "At running model, we thought that little batch size will helpful for performance and generalization, so we used only 77 batch size, but 20000 epochs. while running, we check training accuracy each 400 iteration, if accuracy becomes higher than one threshold, we changed mode and we decayed learning rate. We made 8 accuracy threshold, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8 and each step, learning rate becomes 3 times smaller than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FENglAuzvN-a"
   },
   "source": [
    "_Tell us here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Ki9r2b9vN-c"
   },
   "source": [
    "### Test Set - Do this only once\n",
    "Now that you've gotten a result that you're happy with, test your final model on the test set. This would be the score you would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFe9dhMcvN-d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_checkpoints/my_model_final\n",
      "(Loss, Accuracy) on Test Dataset (1.0317, 0.72)\n"
     ]
    }
   ],
   "source": [
    "# Clear old variables\n",
    "tf.reset_default_graph()  \n",
    "\n",
    "with tf.Session(config=conf) as sess:\n",
    "    #Load your final model\n",
    "    model = my_model_simple1()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"./model_checkpoints/my_model_final\")\n",
    "    print(\"(Loss, Accuracy) on Test Dataset (%.4f, %.2f)\" % run_model_v2(sess, model, X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3-Copy1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
